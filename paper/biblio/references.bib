@ARTICLE{HoB2002,
author={Tin Kam Ho and M. Basu},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Complexity measures of supervised classification problems},
year={2002},
volume={24},
number={3},
pages={289--300},
keywords={pattern classification;computational complexity;geometry;complexity measures;supervised classification problems;geometrical complexity;class boundary;competence domain;static classifier selection;dynamic classifier selection;feature vector confinement;feature vector projection;feature vector transformations;Labeling;Extraterrestrial measurements},
doi={10.1109/34.990132},
ISSN={0162-8828},
month={March}}


@ARTICLE{SongGS2018,
author={Q. Song and Y. Guo and Martin Shepperd},
journal={IEEE Transactions on Software Engineering},
title={A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction},
year={2018},
volume={},
number={},
pages={In press},
keywords={Software;Measurement;Boosting;Machine learning algorithms;Bagging;Computer bugs;Software defect prediction;bug prediction;imbalanced learning;imbalance ratio;effect size},
doi={10.1109/TSE.2018.2836442},
ISSN={0098-5589},
month={}
}


@inproceedings{RodriguezHHDR2014,
 author = {Rodriguez, Daniel and Herraiz, Israel and Harrison, Rachel and Dolado, Javier and Riquelme, Jos{\'e} C.},
 title = {Preliminary Comparison of Techniques for Dealing with Imbalance in Software Defect Prediction},
 booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering (EASE'14)},
 series = {EASE'14},
 year = {2014},
 isbn = {978-1-4503-2476-2},
 location = {London, England, United Kingdom},
 pages = {43:1--43:10},
 articleno = {43},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2601248.2601294},
 doi = {10.1145/2601248.2601294},
 acmid = {2601294},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data quality, defect prediction, imbalanced data},
} 

@incollection{Rice1976,
title = "The Algorithm Selection Problem",
editor = "Morris Rubinoff and Marshall C. Yovits",
series = "Advances in Computers",
publisher = "Elsevier",
volume = "15",
pages = "65--118",
year = "1976",
issn = "0065-2458",
doi = "https://doi.org/10.1016/S0065-2458(08)60520-3",
url = "http://www.sciencedirect.com/science/article/pii/S0065245808605203",
author = "John R. Rice"
}


@article{NogueiraSB2017,
 author = {Nogueira, Sarah and Sechidis, Konstantinos and Brown, Gavin},
 title = {On the Stability of Feature Selection Algorithms},
 journal = {Journal of Machine Learning Research},
 issue_date = {January 2017},
 volume = {18},
 number = {1},
 month = jan,
 year = {2017},
 issn = {1532-4435},
 pages = {6345--6398},
 numpages = {54},
 url = {http://dl.acm.org/citation.cfm?id=3122009.3242031},
 acmid = {3242031},
 publisher = {JMLR.org},
 keywords = {feature selection, stability},
} 

@book{BrazdilGSV2008,
 author = {Brazdil, Pavel and Giraud-Carrier, Christophe and Soares, Carlos and Vilalta, Ricardo},
 title = {Metalearning: Applications to Data Mining},
 year = {2008},
 isbn = {3540732624, 9783540732624},
 edition = {1},
 publisher = {Springer Publishing Company, Incorporated},
}

@misc{lorena2018,
    title={How Complex is your classification problem? A survey on measuring classification complexity},
    author={Ana C. Lorena and Lu\'is P. F. Garcia and Jens Lehmann and Marcilio C. P. Souto and Tin K. Ho},
    year={2018},
    eprint={1808.03591},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@InProceedings{FernandezBA2016,
author="Mor{\'a}n-Fern{\'a}ndez, Laura
and Bol{\'o}n-Canedo, Ver{\'o}nica
and Alonso-Betanzos, Amparo",
editor="Luaces , Oscar
and G{\'a}mez, Jos{\'e} A.
and Barrenechea, Edurne
and Troncoso, Alicia
and Galar, Mikel
and Quinti{\'a}n, H{\'e}ctor
and Corchado, Emilio",
title="Selection of the Best Base Classifier in One-Versus-One Using Data Complexity Measures",
booktitle="Advances in Artificial Intelligence",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="110--120",
abstract="When dealing with multiclass problems, the most used approach is the one based on multiple binary classifiers. This approach consists of employing class binarization techniques which transforms the multiclass problem into a series of binary problems which are solved individually. Then, the resultant predictions are combined to obtain a final solution. A question arises: should the same classification algorithm be used on all binary subproblems? Or should each subproblem be tuned independently? This paper proposes a method to select a different classifier in each binary subproblem---following the one-versus-one strategy---based on the analysis of the theoretical complexity of each subproblem. The experimental results on 12 real world datasets corroborate the adequacy of the proposal when the subproblems have different structure.",
isbn="978-3-319-44636-3"
}

@misc{vanschoren2018,
    title={Meta-Learning: A Survey},
    author={Joaquin Vanschoren},
    year={2018},
    eprint={1810.03548},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@book{BasuH2006,
  doi = {10.1007/978-1-84628-172-3},
  url = {https://doi.org/10.1007/978-1-84628-172-3},
  year = {2006},
  publisher = {Springer London},
  editor = {Mitra Basu and Tin Kam Ho},
  title = {Data Complexity in Pattern Recognition}
}

@article{FriedmanRafsky,
 author = {Friedman, J.H. and Rafsky, L.C},
 title={Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests},
 journal = {The Annals of Statistics},
 volume = {7},
 number = {4},
 pages = {697--717, 1979},
}

@article{FWSmith,
 author = {Smith, F.W.},
 title={Pattern Classifier Design by Linear Programming},
 journal = {IEEE Trans. Computers},
 volume = {17},
 number = {4},
 pages = {367--372},
} 

@article{Santafe2015,
  doi = {10.1007/s10462-015-9433-y},
  url = {https://doi.org/10.1007/s10462-015-9433-y},
  year = {2015},
  month = jun,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {44},
  number = {4},
  pages = {467--508},
  author = {Guzman Santafe and I{\~{n}}aki Inza and Jose A. Lozano},
  title = {Dealing with the evaluation of supervised classification algorithms},
  journal = {Artificial Intelligence Review}
}

@article{Lorena2019,
abstract = {Characteristics extracted from the training datasets of classification problems have proven to be effective predictors in a number of meta-analyses. Among them, measures of classification complexity can be used to estimate the difficulty in separating the data points into their expected classes. Descriptors of the spatial distribution of the data and estimates of the shape and size of the decision boundary are among the known measures for this characterization. This information can support the formulation of new data-driven preprocessing and pattern recognition techniques, which can in turn be focused on challenges highlighted by such characteristics of the problems. This article surveys and analyzes measures that can be extracted from the training datasets to characterize the complexity of the respective classification problems. Their use in recent literature is also reviewed and discussed, allowing to prospect opportunities for future work in the area. Finally, descriptions are given on an R package named Extended Complexity Library (ECoL) that implements a set of complexity measures and is made publicly available.},
archivePrefix = {arXiv},
arxivId = {1808.03591},
author = {Lorena, Ana C. and Garcia, Lu{\'{i}}s P.F. and Lehmann, Jens and Souto, Marcilio C.P. and Ho, Tin K.A.M.},
doi = {10.1145/3347711},
eprint = {1808.03591},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Classification,Complexity measures,Supervised machine learning},
mendeley-groups = {Evaluation},
month = {aug},
number = {5},
title = {{How complex is your classification problem?: A survey on measuring classification complexity}},
url = {https://arxiv.org/abs/1808.03591},
volume = {52},
year = {2019}
}

@article{Lorena2017,
author = {Lorena, Ana and Maciel, Aron and Miranda, Péricles and Costa, Ivan and Prudêncio, Ricardo},
year = {2017},
month = {12},
pages = {},
title = {Data complexity meta-features for regression problems},
journal = {Machine Learning},
doi = {10.1007/s10994-017-5681-1}
}

@article{Oreski2017,
abstract = {While extensive research in data mining has been devoted to developing better feature selection techniques, none of this research has examined the intrinsic relationship between dataset characteristics and a feature selection technique's performance. Thus, our research examines experimentally how dataset characteristics affect both the accuracy and the time complexity of feature selection. To evaluate the performance of various feature selection techniques on datasets of different characteristics, extensive experiments with five feature selection techniques, three types of classification algorithms, seven types of dataset characterization methods and all possible combinations of dataset characteristics are conducted on 128 publicly available datasets. We apply the decision tree method to evaluate the interdependencies between dataset characteristics and performance. The results of the study reveal the intrinsic relationship between dataset characteristics and feature selection techniques' performance. Additionally, our study contributes to research in data mining by providing a roadmap for future research on feature selection and a significantly wider framework for comparative analysis.},
author = {Oreski, Dijana and Oreski, Stjepan and Klicek, Bozidar},
doi = {10.1016/j.asoc.2016.12.023},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Comparative analysis,Data sparsity,Dataset characteristics,Feature noise,Feature selection},
mendeley-groups = {Metalearning},
month = {mar},
pages = {109--119},
publisher = {Elsevier},
title = {{Effects of dataset characteristics on the performance of feature selection techniques}},
url = {https://www.sciencedirect.com/science/article/pii/S156849461630641X},
volume = {52},
year = {2017}
}

@book{molnar2019,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  note       = {\url{https://christophm.github.io/interpretable-ml-book/}},
  year       = {2019},
  subtitle   = {A Guide for Making Black Box Models Explainable}
}


@InProceedings{Xu2018,
  author    = {Zhou Xu and Jin Liu and Xiapu Luo and Tao Zhang},
  title     = {Cross-version defect prediction via hybrid active learning with kernel principal component analysis},
  booktitle = {2018 {IEEE} 25th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
  year      = {2018},
  month     = {mar},
  publisher = {{IEEE}},
  doi       = {10.1109/saner.2018.8330210},
}

@InProceedings{Wang2016,
  author    = {Song Wang and Taiyue Liu and Lin Tan},
  title     = {Automatically learning semantic features for defect prediction},
  booktitle = {Proceedings of the 38th International Conference on Software Engineering - {ICSE} {\textquotesingle}16},
  year      = {2016},
  publisher = {{ACM} Press},
  doi       = {10.1145/2884781.2884804},
}

@Article{Mateos-Garcia2012,
  author    = {Daniel Mateos-Garc{\'{\i}}a and Jorge Garc{\'{\i}}a-Guti{\'{e}}rrez and Jos{\'{e}} C. Riquelme-Santos},
  title     = {On the evolutionary optimization of k-{NN} by label-dependent feature weighting},
  journal   = {Pattern Recognition Letters},
  year      = {2012},
  volume    = {33},
  number    = {16},
  pages     = {2232--2238},
  month     = {dec},
  doi       = {10.1016/j.patrec.2012.08.011},
  publisher = {Elsevier {BV}},
}

@Article{Xia2016,
  author    = {Xin Xia and David Lo and Sinno Jialin Pan and Nachiappan Nagappan and Xinyu Wang},
  title     = {{HYDRA}: Massively Compositional Model for Cross-Project Defect Prediction},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {2016},
  volume    = {42},
  number    = {10},
  pages     = {977--998},
  month     = {oct},
  doi       = {10.1109/tse.2016.2543218},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Chidamber1994,
  author    = {S.R. Chidamber and C.F. Kemerer},
  title     = {A metrics suite for object oriented design},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {1994},
  volume    = {20},
  number    = {6},
  pages     = {476--493},
  month     = {jun},
  doi       = {10.1109/32.295895},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Pedregosa2012,
  author       = {Fabian Pedregosa and Gaël Varoquaux and Alexandre Gramfort and Vincent Michel and Bertrand Thirion and Olivier Grisel and Mathieu Blondel and Andreas Müller and Joel Nothman and Gilles Louppe and Peter Prettenhofer and Ron Weiss and Vincent Dubourg and Jake Vanderplas and Alexandre Passos and David Cournapeau and Matthieu Brucher and Matthieu Perrot and Édouard Duchesnay},
  title        = {Scikit-learn: Machine Learning in Python},
  abstract     = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.org.},
  date         = {2012-01-02},
  eprint       = {http://arxiv.org/abs/1201.0490v4},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1201.0490v4:PDF},
  journaltitle = {Journal of Machine Learning Research (2011)},
  keywords     = {cs.LG, cs.MS},
}

@Article{Dam2018,
  author      = {Hoa Khanh Dam and Trang Pham and Shien Wee Ng and Truyen Tran and John Grundy and Aditya Ghose and Taeksu Kim and Chul-Joo Kim},
  title       = {A deep tree-based model for software defect prediction},
  abstract    = {Defects are common in software systems and can potentially cause various problems to software users. Different methods have been developed to quickly predict the most likely locations of defects in large code bases. Most of them focus on designing features (e.g. complexity metrics) that correlate with potentially defective code. Those approaches however do not sufficiently capture the syntax and different levels of semantics of source code, an important capability for building accurate prediction models. In this paper, we develop a novel prediction model which is capable of automatically learning features for representing source code and using them for defect prediction. Our prediction system is built upon the powerful deep learning, tree-structured Long Short Term Memory network which directly matches with the Abstract Syntax Tree representation of source code. An evaluation on two datasets, one from open source projects contributed by Samsung and the other from the public PROMISE repository, demonstrates the effectiveness of our approach for both within-project and cross-project predictions.},
  date        = {2018-02-03},
  eprint      = {http://arxiv.org/abs/1802.00921v1},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1802.00921v1:PDF},
  keywords    = {cs.SE},
}

@Book{Rijsbergen1979,
  title     = {Information Retrieval},
  publisher = {Butterworth-Heinemann},
  year      = {1979},
  author    = {C. J. Van Rijsbergen},
  isbn      = {0408709294},
  url       = {https://www.amazon.com/Information-Retrieval-C-Van-Rijsbergen/dp/0408709294?SubscriptionId=0JYN1NVW651KCA56C102&tag=techkie-20&linkCode=xm2&camp=2025&creative=165953&creativeASIN=0408709294},
}

@InProceedings{Jureczko2010,
  author    = {Marian Jureczko and Lech Madeyski},
  title     = {Towards identifying software project clusters with regard to defect prediction},
  booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering - {PROMISE} {\textquotesingle}10},
  year      = {2010},
  publisher = {{ACM} Press},
  doi       = {10.1145/1868328.1868342},
}

@article{MadeyskiJ2015,
  author    = {Madeyski, Lech and Hrysko, Jaroslaw},
  title     = {Bottlenecks in Software Defect Prediction Implementation in Industrial Projects},
  year      = {2015},
  volume    = {40},
  number    = {1},
  doi       = {10.1515/fcds-2015-0002}
}

@Article{Bansiya2002,
  author    = {J. Bansiya and C.G. Davis},
  title     = {A hierarchical model for object-oriented design quality assessment},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {2002},
  volume    = {28},
  number    = {1},
  pages     = {4--17},
  doi       = {10.1109/32.979986},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Book{Henderson-Sellers1995,
  title     = {Object-Oriented Metrics: Measures of Complexity},
  publisher = {ADDISON WESLEY PUB CO INC},
  year      = {1995},
  author    = {Henderson-Sellers},
  isbn      = {0132398729},
  date      = {1995-12-11},
  ean       = {9780132398725},
  pagetotal = {252},
  url       = {https://www.ebook.de/de/product/6141164/henderson_sellers_object_oriented_metrics_measures_of_complexity.html},
}

@InProceedings{Tang,
  author    = {Mei-Huei Tang and Ming-Hung Kao and Mei-Hwa Chen},
  title     = {An empirical study on object-oriented metrics},
  booktitle = {Proceedings Sixth International Software Metrics Symposium (Cat. No.{PR}00403)},
  publisher = {{IEEE} Comput. Soc},
  doi       = {10.1109/metric.1999.809745},
}

@inproceedings{Martin1994,
    author = {Martin, Robert},
    booktitle = {Proceedings Workshop Pragmatic and Theoretical Directions in Object-Oriented Software Metrics},
    pages = {151--170},
    title = {{OO Design Quality Metrics - An Analysis of Dependencies}},
    year = {1994},
}

@Article{McCabe1976,
  author    = {T.J. McCabe},
  title     = {A Complexity Measure},
  journal   = {{IEEE} Transactions on Software Engineering},
  year      = {1976},
  volume    = {{SE}-2},
  number    = {4},
  pages     = {308--320},
  month     = {dec},
  doi       = {10.1109/tse.1976.233837},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Turhan2009,
  author    = {Burak Turhan and Tim Menzies and Ay{\c{s}}e B. Bener and Justin Di Stefano},
  title     = {On the relative value of cross-company and within-company data for defect prediction},
  journal   = {Empirical Software Engineering},
  year      = {2009},
  volume    = {14},
  number    = {5},
  pages     = {540--578},
  month     = {jan},
  doi       = {10.1007/s10664-008-9103-7},
  publisher = {Springer Nature},
}

@misc{promiserepo,
  authors =  {Menzies, T., Krishna, R., Pryor, D.},
  year =   {2015},
  title =  {The Promise Repository of Empirical Software Engineering Data},
  notes =  {http://openscience.us/repo. North Carolina State University, Department of Computer Science},
}

@Article{Limsettho2018,
  author    = {Nachai Limsettho and Kwabena Ebo Bennin and Jacky W. Keung and Hideaki Hata and Kenichi Matsumoto},
  title     = {Cross project defect prediction using class distribution estimation and oversampling},
  journal   = {Information and Software Technology},
  year      = {2018},
  volume    = {100},
  pages     = {87--102},
  month     = {aug},
  doi       = {10.1016/j.infsof.2018.04.001},
  publisher = {Elsevier {BV}},
}

@Article{Menzies2010,
  author    = {Tim Menzies and Zach Milton and Burak Turhan and Bojan Cukic and Yue Jiang and Ay{\c{s}}e Bener},
  title     = {Defect prediction from static code features: current~results, limitations, new approaches},
  journal   = {Automated Software Engineering},
  year      = {2010},
  volume    = {17},
  number    = {4},
  pages     = {375--407},
  month     = {may},
  doi       = {10.1007/s10515-010-0069-5},
  publisher = {Springer Nature},
}

@Article{Fenton1994,
  author    = {N. Fenton and S.L. Pfleeger and R.L. Glass},
  title     = {Science and substance: a challenge to software engineers},
  journal   = {{IEEE} Software},
  year      = {1994},
  volume    = {11},
  number    = {4},
  pages     = {86--95},
  month     = {jul},
  doi       = {10.1109/52.300094},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{He2011,
  author    = {Zhimin He and Fengdi Shu and Ye Yang and Mingshu Li and Qing Wang},
  title     = {An investigation on the feasibility of cross-project defect prediction},
  journal   = {Automated Software Engineering},
  year      = {2011},
  volume    = {19},
  number    = {2},
  pages     = {167--199},
  month     = {jul},
  doi       = {10.1007/s10515-011-0090-3},
  publisher = {Springer Nature},
}

@InProceedings{Herbold2013,
  author    = {Steffen Herbold},
  title     = {Training data selection for cross-project defect prediction},
  booktitle = {Proceedings of the 9th International Conference on Predictive Models in Software Engineering - {PROMISE} {\textquotesingle}13},
  year      = {2013},
  publisher = {{ACM} Press},
  doi       = {10.1145/2499393.2499395},
}

@InProceedings{Nam2013,
  author    = {Jaechang Nam and Sinno Jialin Pan and Sunghun Kim},
  title     = {Transfer defect learning},
  booktitle = {2013 35th International Conference on Software Engineering ({ICSE})},
  year      = {2013},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/icse.2013.6606584},
}

@InProceedings{Peters2013,
  author    = {Fayola Peters and Tim Menzies and Andrian Marcus},
  title     = {Better cross company defect prediction},
  booktitle = {2013 10th Working Conference on Mining Software Repositories ({MSR})},
  year      = {2013},
  month     = {may},
  publisher = {{IEEE}},
  doi       = {10.1109/msr.2013.6624057},
}

@Article{Benavoli2016,
  author      = {Alessio Benavoli and Giorgio Corani and Janez Demsar and Marco Zaffalon},
  title       = {Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis},
  abstract    = {The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.},
  date        = {2016-06-14},
  eprint      = {http://arxiv.org/abs/1606.04316v3},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1606.04316v3:PDF},
  keywords    = {stat.ML, cs.LG},
}

@article{Matthews1975,
  doi = {10.1016/0005-2795(75)90109-9},
  url = {https://doi.org/10.1016/0005-2795(75)90109-9},
  year = {1975},
  month = oct,
  publisher = {Elsevier {BV}},
  volume = {405},
  number = {2},
  pages = {442--451},
  author = {B.W. Matthews},
  title = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
  journal = {Biochimica et Biophysica Acta ({BBA}) - Protein Structure}
}

@article{Fawcett2006,
  doi = {10.1016/j.patrec.2005.10.010},
  url = {https://doi.org/10.1016/j.patrec.2005.10.010},
  year = {2006},
  month = jun,
  publisher = {Elsevier {BV}},
  volume = {27},
  number = {8},
  pages = {861--874},
  author = {Tom Fawcett},
  title = {An introduction to {ROC} analysis},
  journal = {Pattern Recognition Letters}
}

@article{Zhang03,
  url = {https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf},
  year = {2003},
  author = {Zhang, J. and Mani, I.},
  booktitle = {Proceedings of the ICML'2003 Workshop on Learning from Imbalanced Datasets},
  title = {KNN Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction}
}

@Article{Afzal2012IJSEKE,
  Title                    = {Resampling methods in software quality classification},
  Author                   = {Wasif Afzal and Richard Torkar and Robert Feldt},
  Journal                  = {International Journal of Software Engineering and Knowledge Engineering},
  Year                     = {2012},
  Pages                    = {203--223},
  Volume                   = {22},

  Issue                    = {2},
  Keywords                 = {Software quality; Prediction; Resampling; Empirical study},
  Publisher                = {World Scientific Publishing Company}
}


@Article{AKA91,
  Title                    = {Instance-based Learning Algorithms},
  Author                   = {D.W. Aha and D. Kibler and M.K. Albert},
  Journal                  = {Machine Learning},
  Year                     = {1991},
  Pages                    = {37--66},
  Volume                   = {6}
}



@Article{Breiman01,
  Title                    = {Random Forests},
  Author                   = {Leo Breiman},
  Journal                  = {Machine Learning},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {5--32},
  Volume                   = {45},

  Address                  = {Hingham, MA, USA},
  Doi                      = {http://dx.doi.org/10.1023/A:1010933404324},
  ISSN                     = {0885-6125},
  Publisher                = {Kluwer Academic Publishers}
}

@Article{Breiman96,
  Title                    = {Bagging predictors},
  Author                   = {Breiman, Leo},
  Journal                  = {Machine Learning},
  Year                     = {1996},
  Pages                    = {123-140},
  Volume                   = {24},

  Doi                      = {10.1007/BF00058655},
  ISSN                     = {0885-6125},
  Issue                    = {2},
  Keywords                 = {Aggregation; Bootstrap; Averaging; Combining},
  Language                 = {English},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1007/BF00058655}
}

@Book{Breiman1984,
  Title                    = {Classification and Regression Trees},
  Author                   = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
  Publisher                = {Wadsworth International Group},
  Year                     = {1984},
  Address                  = {Belmont, California},
  Pages                    = {1--368},
  Isbn                     = {0412048418, 9780412048418},
  Note = {\url{https://books.google.es/books/about/Classification_and_Regression_Trees.html?id=JwQx-WOmSyQC&redir_esc=y}}
}

@Article{Byrt1993,
  Title                    = {{Bias, prevalence and kappa}},
  Author                   = {Byrt, Ted and Bishop, Janet and Carlin, John B.},
  Journal                  = {Journal of Clinical Epidemiology},
  Year                     = {1993},
  Number                   = {5},
  Pages                    = {423--429},
  Volume                   = {46},

  Abstract                 = {Since the introduction of Cohen's kappa as a chance-adjusted measure of agreement between two observers, several “paradoxes” in its interpretation have been pointed out. The difficulties occur because kappa not only measures agreement but is also affected in complex ways by the presence of bias between observers and by the distributions of data across the categories that are used (“prevalence”). In this paper, new indices that provide independent measures of bias and prevalence, as well as of observed agreement, are defined and a simple formula is derived that expresses kappa in terms of these three indices. When comparisons are made between agreement studies it can be misleading to report kappa values alone, and it is recommended that researchers also include quantitative indicators of bias and prevalence.},
  Keywords                 = {Agreement,Bias,Kappa,Prevalence},
  Url                      = {http://www.sciencedirect.com/science/article/pii/089543569390018V}
}



@Article{Catal_WIRED12,
  Title                    = {Software mining and fault prediction},
  Author                   = {Catal, Cagatay},
  Journal                  = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {420--426},
  Volume                   = {2},

  Abstract                 = {Mining software repositories (MSRs) such as source control repositories, bug repositories, deployment logs, and code repositories provide useful patterns for practitioners. Instead of using these repositories as record-keeping ones, we need to transform them into active repositories that can guide the decision processes inside the company. By MSRs with several data mining algorithms, effective software fault prediction models can be built and error-prone modules can be detected prior to the testing phase. We discuss numerous real-world challenges in building accurate fault prediction models and present some solutions to these challenges. ? 2012 Wiley Periodicals, Inc.},
  Doi                      = {10.1002/widm.1067},
  ISSN                     = {1942-4795},
  Publisher                = {John Wiley \& Sons, Inc.},
  Url                      = {http://dx.doi.org/10.1002/widm.1067}
}

@Article{CC2009,
  Title                    = {A systematic review of software fault prediction studies},
  Author                   = {Cagatay Catal and Banu Diri},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {7346 - 7354},
  Volume                   = {36},

  Abstract                 = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
  Doi                      = {10.1016/j.eswa.2008.10.027},
  ISSN                     = {0957-4174},
  Keywords                 = {Machine learning},
  Owner                    = {drg},
  Timestamp                = {2012.09.15},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417408007215}
}

@Article{CBHK2002,
  Title                    = {SMOTE: Synthetic Minority Over-sampling Technique},
  Author                   = {N.V. Chawla and K.W. Bowyer and L.O. Hall and W.P. Kegelmeyer},
  Journal                  = {Journal of Artificial Intelligence Research },
  Year                     = {2002},
  Pages                    = {321--357},
  Volume                   = {16}
}

@article{sorensen1948,
  Title                    = {A method of establishing groups of equal amplitude in plant sociology based on similarity of species and its application to analyses of the vegetation on Danish commons},
  Author                   = {S{\o}rensen, T.},
  Year                     = {1948},
  Pages                    = {1--34},
  Volume                   = {5(4)}
}

@article{dice1945,
  Title                    = {Measures of the Amount of Ecologic Association Between Species},
  Author                   = {Dice, Lee R.},
  Journal                  = {Ecology},
  Year                     = {1945},
  Pages                    = {297--302},
  Volume                   = {26(3)}
}

@InProceedings{CLHB2003,
  Title                    = {SMOTEBoost: Improving prediction of the minority class in boosting},
  Author                   = {N.V. Chawla and A. Lazarevic and L.O. Hall and K.W. Bowyer},
  Booktitle                = {7th European Conference on Principles and Practice of Knowledge Discovery in Databases({PKDD 2003})},
  Year                     = {2003},
  Pages                    = {107--119},

  City                     = {Cavtat Dubrovnik},
  Country                  = {Croatia}
}

@Article{ChawlaBHK02,
  Title                    = {SMOTE: Synthetic Minority Over-sampling Technique},
  Author                   = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
  Journal                  = {J. Artif. Intell. Res. (JAIR)},
  Year                     = {2002},
  Pages                    = {321-357},
  Volume                   = {16},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1613/jair.953}
}

@Misc{clauset07,
  Title                    = {Power-law distributions in empirical data},

  Author                   = {Aaron Clauset and Cosma Rohilla Shalizi and M.~E.~J. Newman},
  Year                     = {2007},

  Url                      = {http://www.citebase.org/abstract?id=oai:arXiv.org:0706.1062}
}

@InProceedings{cohen1999,
  Title                    = {A simple, fast, and effective rule learner},
  Author                   = {Cohen, W.W. and Singer, Y.},
  Booktitle                = {Proceedings of the National Conference on Artificial Intelligence},
  Year                     = {1999},
  Organization             = {JOHN WILEY \& SONS LTD},
  Pages                    = {335--342}
}

@InProceedings{Cohen1995,
  Title                    = {Fast Effective Rule Induction},
  Author                   = {William W. Cohen},
  Booktitle                = {Twelfth International Conference on Machine Learning},
  Year                     = {1995},
  Pages                    = {115-123},
  Publisher                = {Morgan Kaufmann}
}

@Article{5928349,
  Title                    = {On the Distribution of Bugs in the Eclipse System},
  Author                   = {Concas, G. and Marchesi, M. and Murgia, A. and Tonelli, R. and Turnu, I.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2011},

  Month                    = {Nov-Dec},
  Number                   = {6},
  Pages                    = {872--877},
  Volume                   = {37},

  Doi                      = {10.1109/TSE.2011.54},
  ISSN                     = {0098-5589},
  Keywords                 = {Pareto principle;Weibull cumulative distribution;eclipse system;software systems;statistical perspective;Pareto analysis;Weibull distribution;eclipses;}
}

@Article{Cortes95,
  Title                    = {Support-vector networks},
  Author                   = {Cortes, Corinna and Vapnik, Vladimir},
  Journal                  = {Machine Learning},
  Year                     = {1995},
  Pages                    = {273-297},
  Volume                   = {20},

  Doi                      = {10.1007/BF00994018},
  ISSN                     = {0885-6125},
  Issue                    = {3},
  Keywords                 = {pattern recognition; efficient learning algorithms; neural networks; radial basis function classifiers; polynomial classifiers},
  Language                 = {English},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1007/BF00994018}
}

@Article{CWHW12,
  Title                    = {Free/Libre open-source software development: What we know and what we do not know},
  Author                   = {Crowston, Kevin and Wei, Kangning and Howison, James and Wiggins, Andrea},
  Journal                  = {ACM Computing Surveys},
  Year                     = {2008},

  Month                    = mar,
  Number                   = {2},
  Pages                    = {7:1--7:35},
  Volume                   = {44},

  Acmid                    = {2089127},
  Address                  = {New York, NY, USA},
  Articleno                = {7},
  Doi                      = {10.1145/2089125.2089127},
  ISSN                     = {0360-0300},
  Issue_date               = {February 2012},
  Keywords                 = {Free/Libre open-source software, computer-mediated communication, development, distributed work},
  Numpages                 = {35},
  Publisher                = {ACM}
}

@InProceedings{DAmb2010a,
  Title                    = {An Extensive Comparison of Bug Prediction Approaches},
  Author                   = {Marco D'Ambros and Michele Lanza and Romain Robbes},
  Booktitle                = {Proceedings of the 7th IEEE Working Conference on Mining Software Repositories (MSR07)},
  Year                     = {2010},
  Pages                    = {31 - 41},
  Publisher                = {IEEE CS Press}
}

@Article{ALR11,
  Title                    = {Evaluating defect prediction approaches: a benchmark and an extensive comparison},
  Author                   = {{D'Ambros}, Marco and Lanza, Michele and Robbes, Romain},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2008},
  Note                     = {10.1007/s10664-011-9173-9},
  Pages                    = {1-47},

  Affiliation              = {REVEAL @ Faculty of Informatics, University of Lugano, 6900 Lugano, Switzerland},
  ISSN                     = {1382-3256},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@InProceedings{Davis2006,
  Title                    = {The relationship between Precision-Recall and ROC curves},
  Author                   = {Davis, Jesse and Goadrich, Mark},
  Booktitle                = {Proceedings of the 23rd international conference on Machine learning (ICLM'06},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {233--240},
  Publisher                = {ACM},
  Series                   = {ICML'06},

  Acmid                    = {1143874},
  Doi                      = {10.1145/1143844.1143874},
  ISBN                     = {1-59593-383-2},
  Location                 = {Pittsburgh, Pennsylvania},
  Numpages                 = {8},
  Url                      = {http://doi.acm.org/10.1145/1143844.1143874}
}

@Article{Dean08,
  Title                    = {MapReduce: simplified data processing on large clusters},
  Author                   = {Dean, Jeffrey and Ghemawat, Sanjay},
  Journal                  = {Commun. ACM},
  Year                     = {2008},

  Month                    = jan,
  Number                   = {1},
  Pages                    = {107--113},
  Volume                   = {51},

  Acmid                    = {1327492},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1327452.1327492},
  ISSN                     = {0001-0782},
  Issue_date               = {January 2008},
  Numpages                 = {7},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/1327452.1327492}
}

@Article{Dejaeger_TSE12_EffEst,
  Title                    = {Data Mining Techniques for Software Effort Estimation: A Comparative Study},
  Author                   = {Dejaeger, K. and Verbeke, W. and Martens, D. and Baesens, B.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2012},

  Month                    = {march-april },
  Number                   = {2},
  Pages                    = {375 -397},
  Volume                   = {38},

  Abstract                 = {A predictive model is required to be accurate and comprehensible in order to inspire confidence in a business setting. Both aspects have been assessed in a software effort estimation setting by previous studies. However, no univocal conclusion as to which technique is the most suited has been reached. This study addresses this issue by reporting on the results of a large scale benchmarking study. Different types of techniques are under consideration, including techniques inducing tree/rule-based models like M5 and CART, linear models such as various types of linear regression, nonlinear models (MARS, multilayered perceptron neural networks, radial basis function networks, and least squares support vector machines), and estimation techniques that do not explicitly induce a model (e.g., a case-based reasoning approach). Furthermore, the aspect of feature subset selection by using a generic backward input selection wrapper is investigated. The results are subjected to rigorous statistical testing and indicate that ordinary least squares regression in combination with a logarithmic transformation performs best. Another key finding is that by selecting a subset of highly predictive attributes such as project size, development, and environment related attributes, typically a significant increase in estimation accuracy can be obtained.},
  Doi                      = {10.1109/TSE.2011.55},
  ISSN                     = {0098-5589},
  Keywords                 = {CART;M5;data mining techniques;estimation techniques;feature subset selection;generic backward input selection wrapper;linear regression;logarithmic transformation;nonlinear models;ordinary least squares regression;predictive model;rigorous statistical testing;rule-based models;software effort estimation;data mining;program testing;regression analysis;software cost estimation;}
}

@Article{Demsar2006,
  Title                    = {Statistical Comparisons of Classifiers over Multiple Data Sets},
  Author                   = {Dem\v{s}ar, Janez},
  Journal                  = {Journal of Machine Learning Research},
  Year                     = {2006},

  Month                    = dec,
  Pages                    = {1--30},
  Volume                   = {7},

  Acmid                    = {1248548},
  ISSN                     = {1532-4435},
  Issue_date               = {12/1/2006},
  Numpages                 = {30},
  Publisher                = {JMLR.org},
  Url                      = {http://dl.acm.org/citation.cfm?id=1248547.1248548}
}

@PhdThesis{Desharnais88,
  Title                    = {Analyse statistique de la productivite des projects de development en informatique a partir de la technique des points de fonction},
  Author                   = {J.M. Desharnais},
  School                   = {Univ. du Quebec a Montreal},
  Year                     = {1988},
  Type                     = {MSc Thesis}
}

@Article{DER05,
  Title                    = {Supporting Controlled Experimentation with Testing Techniques: An Infrastructure and its Potential Impact},
  Author                   = {Do, Hyunsook and Elbaum, Sebastian and Rothermel, Gregg},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2005},

  Month                    = {October},
  Pages                    = {405--435},
  Volume                   = {10},

  Acmid                    = {1089928},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1007/s10664-005-3861-2},
  ISSN                     = {1382-3256},
  Issue                    = {4},
  Keywords                 = {Software testing, controlled experimentation, experiment infrastructure, regression testing},
  Numpages                 = {31},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dl.acm.org/citation.cfm?id=1089922.1089928}
}

@Article{Dolado01_CostEst,
  Title                    = {On the problem of the software cost function},
  Author                   = {J.J Dolado},
  Journal                  = {Information and Software Technology},
  Year                     = {2001},
  Number                   = {1},
  Pages                    = {61--72},
  Volume                   = {43},

  Abstract                 = {The question of finding a function for software cost estimation is a long-standing issue in the software engineering field. The results of other works have shown different patterns for the unknown function, which relates software size to project cost (effort). In this work, the research about this problem has been made by using the technique of Genetic Programming (GP) for exploring the possible cost functions. Both standard regression analysis and GP have been applied and compared on several data sets. However, regardless of the method, the basic size-effort relationship does not show satisfactory results, from the predictive point of view, across all data sets. One of the results of this work is that we have not found significant deviations from the linear model in the software cost functions. This result comes from the marginal cost analysis of the equations with best predictive values.},
  Doi                      = {10.1016/S0950-5849(00)00137-3},
  ISSN                     = {0950-5849},
  Keywords                 = {Software cost function},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584900001373}
}

@Article{Dolado97,
  Title                    = {A study of the relationships among Albrecht and Mark II Function Points, lines of code 4GL and effort},
  Author                   = {J.J. Dolado},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1997},
  Number                   = {2},
  Pages                    = {161--173},
  Volume                   = {37},

  Abstract                 = {There is a strong interest in finding metrics for replacing the common LOC measure of software size, with most of the interest focusing on the Function Point measures. Mark II Function Points were proposed as a better technique than the original of Albrecht Function Points. In this work, the results of a study comparing those measures are stated, and they are also compared against effort and LOC. Since other published results are based on samples generated randomly, it is interesting to see both methods working when applied to the same projects. The data collected comes from the measurements of academic projects. The fact that all projects have been developed in the same environment (mostly 4GL) and domain (accounting information systems) allows the value of the technical complexity adjustment\9D variable to be set to constant and also allows us to examine the relationships among the variables. Several conclusions are reported.},
  Doi                      = {10.1016/S0164-1212(96)00111-2},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121296001112}
}

@InProceedings{Domingos1999,
  Title                    = {MetaCost: a general method for making classifiers cost-sensitive},
  Author                   = {Domingos, Pedro},
  Booktitle                = {Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining},
  Year                     = {1999},

  Address                  = {New York, NY, USA},
  Pages                    = {155--164},
  Publisher                = {ACM},
  Series                   = {KDD '99},

  __markedentry            = {[drg:6]},
  Acmid                    = {312220},
  Doi                      = {10.1145/312129.312220},
  ISBN                     = {1-58113-143-7},
  Location                 = {San Diego, California, United States},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/312129.312220}
}

@Article{Elish2008,
  Title                    = {Predicting defect-prone software modules using support vector machines },
  Author                   = {Karim O. Elish and Mahmoud O. Elish},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {649--660},
  Volume                   = {81},

  Abstract                 = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of \{SVM\} in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four \{NASA\} datasets. The results indicate that the prediction performance of \{SVM\} is generally better than, or at least, is competitive against the compared models. },
  Doi                      = {http://dx.doi.org/10.1016/j.jss.2007.07.040},
  ISSN                     = {0164-1212},
  Keywords                 = {Software metrics},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S016412120700235X}
}



@Article{FPS96,
  Title                    = {The KDD process for extracting useful knowledge from volumes of data},
  Author                   = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
  Journal                  = {Communications of the ACM},
  Year                     = {1996},

  Month                    = {November},
  Pages                    = {27--34},
  Volume                   = {39},

  Acmid                    = {240464},
  Address                  = {New York, NY, USA},
  Doi                      = {http://doi.acm.org/10.1145/240455.240464},
  ISSN                     = {0001-0782},
  Issue                    = {11},
  Numpages                 = {8},
  Publisher                = {ACM}
}

@Article{Fenton99,
  Title                    = {A critique of software defect prediction models},
  Author                   = {Fenton, N.E. and Neil, M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {1999},

  Month                    = {sep/oct},
  Number                   = {5},
  Pages                    = {675--689},
  Volume                   = {25},

  Abstract                 = {Many organizations want to predict the number of defects (faults) in software systems, before they are deployed, to gauge the likely delivered quality and maintenance effort. To help in this numerous software metrics and statistical models have been developed, with a correspondingly large literature. We provide a critical review of this literature and the state-of-the-art. Most of the wide range of prediction models use size and complexity metrics to predict defects. Others are based on testing data, the ldquo;quality rdquo; of the development process, or take a multivariate approach. The authors of the models have often made heroic contributions to a subject otherwise bereft of empirical studies. However, there are a number of serious theoretical and practical problems in many studies. The models are weak because of their inability to cope with the, as yet, unknown relationship between defects and failures. There are fundamental statistical and data quality problems that undermine model validity. More significantly many prediction models tend to model only part of the underlying problem and seriously misspecify it. To illustrate these points the Goldilock's Conjecture, that there is an optimum module size, is used to show the considerable problems inherent in current defect prediction approaches. Careful and considered analysis of past and new results shows that the conjecture lacks support and that some models are misleading. We recommend holistic models for software defect prediction, using Bayesian belief networks, as alternative approaches to the single-issue models used at present. We also argue for research into a theory of ldquo;software decomposition rdquo; in order to test hypotheses about defect introduction and help construct a better science of software engineering},
  Doi                      = {10.1109/32.815326},
  ISSN                     = {0098-5589},
  Keywords                 = {Bayesian belief networks;holistic models;literature review;multivariate approach;software decomposition;software defect prediction models;software engineering;software maintenance;software metrics;software quality;statistical models;belief networks;program testing;software maintenance;software metrics;software quality;software reliability;}
}

@InProceedings{FGH11,
  Title                    = {Addressing the Classification with Imbalanced Data: Open Problems and New Challenges on Class Distribution},
  Author                   = {Alberto Fern\'andez and Salvador Garc\'ia and Francisco Herrera},
  Booktitle                = {6th International Conference on Hybrid Artificial Intelligence Systems (HAIS)},
  Year                     = {2011},
  Pages                    = {1--10},

  Bibsource                = {DBLP, http://dblp.uni-trier.de},
  Ee                       = {http://dx.doi.org/10.1007/978-3-642-21219-2_1}
}



@InProceedings{FlachHR11,
  Title                    = {A Coherent Interpretation of AUC as a Measure of Aggregated Classification Performance},
  Author                   = {Peter A. Flach and Jos{\'e} Hern{\'a}ndez-Orallo and C{\`e}sar Ferri Ramirez},
  Booktitle                = {Proceedings of the 28th International Conference on Machine Learning (ICML'11)},
  Year                     = {2011},

  Address                  = {Bellevue, Washington, USA},
  Month                    = {June 28- July 2, 2011},
  Pages                    = {657--664},

  Bibsource                = {DBLP, http://dblp.uni-trier.de}
}

@Article{Forman:2010:ACS:1882471.1882479,
  Title                    = {Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement},
  Author                   = {Forman, George and Scholz, Martin},
  Journal                  = {SIGKDD Explor. Newsl.},
  Year                     = {2010},

  Month                    = nov,
  Number                   = {1},
  Pages                    = {49--57},
  Volume                   = {12},

  Acmid                    = {1882479},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1882471.1882479},
  ISSN                     = {1931-0145},
  Issue_date               = {June 2010},
  Numpages                 = {9},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/1882471.1882479}
}

@Article{FS1997,
  Title                    = {A decision-theoretic generalization of on-line learning and an application to boosting},
  Author                   = {Y. Freund and R.E. Schapire},
  Journal                  = {Journal of Computer and System Sciences},
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {119--139},
  Volume                   = {55}
}

@InProceedings{Freund1996,
  Title                    = {Experiments with a new boosting algorithm},
  Author                   = {Yoav Freund and Robert E. Schapire},
  Booktitle                = {Thirteenth International Conference on Machine Learning},
  Year                     = {1996},

  Address                  = {San Francisco},
  Pages                    = {148-156},
  Publisher                = {Morgan Kaufmann}
}

@Article{Galar2013,
  Title                    = {{EUSboost}: Enhancing Ensembles for Highly Imbalanced Data-sets by Evolutionary Undersampling},
  Author                   = {Galar, Mikel and Fern\'{a}ndez, Alberto and Barrenechea, Edurne and Herrera, Francisco},
  Journal                  = {Pattern Recognition},
  Year                     = {2013},

  Month                    = may,
  Number                   = {null},
  Volume                   = {null},

  Abstract                 = {Classification with imbalanced data-sets has become one of the most challenging problems in Data Mining. Being one class much more represented than the other produces undesirable effects in both the learning and classification processes, mainly regarding the minority class. Such a problem needs accurate tools to be undertaken; lately, ensembles of classifiers have emerged as a possible solution. Among ensemble proposals, the combination of Bagging and Boosting with preprocessing techniques has proved its ability to enhance the classification of the minority class. In this paper, we develop a new ensemble construction algorithm (EUSBoost) based on RUSBoost, one of the simplest and most accurate ensemble, which combines random undersampling with Boosting algorithm. Our methodology aims to improve the existing proposals enhancing the performance of the base classifiers by the usage of the evolutionary undersampling approach. Besides, we promote diversity favoring the usage of different subsets of majority class instances to train each base classifier. Centred on two-class highly imbalanced problems, we will prove, supported by the proper statistical analysis, that EUSBoost is able to outperform the state-of-the-art methods based on ensembles. We will also analyze its advantages using kappa-error diagrams, which we adapt to the imbalanced scenario.},
  Doi                      = {10.1016/j.patcog.2013.05.006},
  ISSN                     = {00313203},
  Keywords                 = {boosting,class distribution,classification,ensembles,imbalanced data-sets,kappa-error diagrams},
  Mendeley-groups          = {ImbalancedData,Slice},
  Url                      = {http://dx.doi.org/10.1016/j.patcog.2013.05.006}
}

@Article{Galar2012,
  Title                    = {A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches},
  Author                   = {Galar, M. and Fern\'andez, A. and Barrenechea, E. and Bustince, H. and Herrera, F.},
  Journal                  = {IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews},
  Year                     = {2012},
  Number                   = {4},
  Pages                    = {463--484},
  Volume                   = {42},

  Abstract                 = {Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.},
  Doi                      = {10.1109/TSMCC.2011.2161285},
  ISSN                     = {1094-6977},
  Keywords                 = {data mining;learning (artificial intelligence);pattern classification;bagging-based approach;boosting-based approach;class imbalance problem;classifier ensemble;classifier learning;data mining community;ensemble learning algorithms;ensemble-based algorithms;ensemble-based method taxonomy;hybrid-based approach;imbalanced data-sets;inner ensemble methodology;machine learning;preprocessing techniques;random undersampling techniques;two-class problems;Accuracy;Algorithm design and analysis;Bagging;Learning systems;Noise;Proposals;Training;Bagging;boosting;class distribution;classification;ensembles;imbalanced data-sets;multiple classifier systems}
}

@InCollection{Garcia2010ICANN,
  Title                    = {Using Evolutionary Multiobjective Techniques for Imbalanced Classification Data},
  Author                   = {Garc\'ia, Sandra and Aler, Ricardo and Galv\'an, In\'es Mar\'ia},
  Booktitle                = {Artificial Neural Networks - ICANN 2010},
  Publisher                = {Springer Berlin Heidelberg},
  Year                     = {2010},
  Editor                   = {Diamantaras, Konstantinos and Duch, Wlodek and Iliadis, Lazaros S.},
  Pages                    = {422--427},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {6352},

  Doi                      = {10.1007/978-3-642-15819-3_57},
  ISBN                     = {978-3-642-15818-6},
  Url                      = {http://dx.doi.org/10.1007/978-3-642-15819-3_57}
}

@InCollection{conformal2014,
title = "Conformal Prediction for Reliable Machine Learning",
editor = "Vineeth N. Balasubramanian and Shen-Shyang Ho and Vladimir Vovk",
booktitle = "Conformal Prediction for Reliable Machine Learning",
publisher = "Morgan Kaufmann",
address = "Boston",
pages = "i",
year = "2014",
isbn = "978-0-12-398537-8",
doi = "https://doi.org/10.1016/B978-0-12-398537-8.00014-6",
url = "http://www.sciencedirect.com/science/article/pii/B9780123985378000146"
}

@Article{GR12,
  Title                    = {On the reproducibility of empirical software engineering studies based on data retrieved from development repositories},
  Author                   = {Gonz\'alez-Barahona, Jes\'us and Robles, Gregorio},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012},
  Note                     = {10.1007/s10664-011-9181-9},
  Pages                    = {75-89},
  Volume                   = {17},

  Affiliation              = {Universidad Rey Juan Carlos, Mostoles, Spain},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands},
  Url                      = {http://dx.doi.org/10.1007/s10664-011-9181-9}
}

@InProceedings{GrayBDSB11,
  Title                    = {The misuse of the NASA metrics data program data sets for automated software defect prediction},
  Author                   = {Gray, David and Bowes, David and Davey, Neil and Sun, Yi and Christianson, Bruce},
  Booktitle                = {Evaluation Assessment in Software Engineering (EASE 2011), 15th Annual Conference on},
  Year                     = {2011},
  Month                    = {april},
  Pages                    = {96 -103},

  Abstract                 = {Background: The NASA Metrics Data Program data sets have been heavily used in software defect prediction experiments. Aim: To demonstrate and explain why these data sets require significant pre-processing in order to be suitable for defect prediction. Method: A meticulously documented data cleansing process involving all 13 of the original NASA data sets. Results: Post our novel data cleansing process; each of the data sets had between 6 to 90 percent less of their original number of recorded values. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: Defect prediction data sets could benefit from lower level code metrics in addition to those more commonly used, as these will help to distinguish modules, reducing the likelihood of repeated data points. Three: The bulk of defect prediction experiments based on the NASA Metrics Data Program data sets may have led to erroneous findings. This is mainly due to repeated data points potentially causing substantial amounts of training and testing data to be identical.},
  Doi                      = {10.1049/ic.2011.0012}
}

@Article{HBBGC11,
  Title                    = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
  Author                   = {Tracy Hall and Sarah Beecham and David Bowes and David Gray and Steve Counsell},
  Journal                  = {Transactions on Software Engineering},
  Year                     = {In Press -- 2011},

  Abstract                 = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs and improve the quality of software. Objective: We investigate how the context of models, the independent variables used and the modelling techniques applied, influence the performance of fault prediction models. Method:We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesise the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modelling techniques such as Naïve Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology and performance comprehensively.}
}

@Book{Halstead:77,
  Title                    = {Elements of software science},
  Author                   = {M.H. Halstead},
  Publisher                = {Elsevier},
  Year                     = {1977},

  Address                  = {New York ; Oxford},
  Series                   = {Elsevier Computer Science Library. Operating And Programming Systems Series; 2}
}

@Article{Hand09,
  Title                    = {Measuring classifier performance: a coherent alternative to the area under the ROC curve},
  Author                   = {Hand, David J.},
  Journal                  = {Mach. Learn.},
  Year                     = {2009},

  Month                    = oct,
  Number                   = {1},
  Pages                    = {103--123},
  Volume                   = {77},

  Acmid                    = {1613009},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1007/s10994-009-5119-5},
  ISSN                     = {0885-6125},
  Issue_date               = {October 2009},
  Keywords                 = {AUC, Classification, Cost, Error rate, Loss, Misclassification rate, ROC curves, Sensitivity, Specificity},
  Numpages                 = {21},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1007/s10994-009-5119-5}
}

@InProceedings{Harman2010,
  Title                    = {The relationship between search based software engineering and predictive modeling},
  Author                   = {Harman, Mark},
  Booktitle                = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {1:1--1:13},
  Publisher                = {ACM},
  Series                   = {PROMISE '10},

  Acmid                    = {1868330},
  Articleno                = {1},
  Doi                      = {http://doi.acm.org/10.1145/1868328.1868330},
  ISBN                     = {978-1-4503-0404-7},
  Location                 = {Timisoara, Romania},
  Numpages                 = {13}
}

@InProceedings{HarmanC04,
  Title                    = {Metrics are fitness functions too},
  Author                   = {Harman, M. and Clark, J.},
  Booktitle                = {Proceedings. 10th International Symposium on Software Metrics 2004},
  Year                     = {2004},
  Month                    = {sept.},
  Pages                    = {58--69},

  Doi                      = {10.1109/METRIC.2004.1357891},
  ISSN                     = {1530-1435},
  Keywords                 = { fitness function; search-based software engineering; software metrics; software validation; genetic algorithms; program verification; software metrics; software process improvement;}
}

@Article{Harman2001,
  Title                    = {Search-based software engineering},
  Author                   = {Mark Harman and Bryan F. Jones},
  Journal                  = {Information and Software Technology},
  Year                     = {2001},
  Number                   = {14},
  Pages                    = {833--839},
  Volume                   = {43},
  ISSN                     = {0950-5849}
}

@Article{Hastings01,
  Title                    = {A vector-based approach to software size measurement and effort estimation},
  Author                   = {Hastings, T.E. and Sajeev, A.S.M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2001},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {337--350},
  Volume                   = {27},

  Abstract                 = {Software size is a fundamental product measure that can be used for assessment, prediction and improvement purposes. However, existing software size measures, such as function points, do not address the underlying problem complexity of software systems adequately. This can result in disproportional measures of software size for different types of systems. We propose a vector size measure (VSM) that incorporates both functionality and problem complexity in a balanced and orthogonal manner. The VSM is used as the input to a vector prediction model (VPM) which can be used to estimate development effort early in the software life-cycle. We theoretically validate the approach against a formal framework. We also empirically validate the approach with a pilot study. The results indicate that the approach provides a mechanism to measure the size of software systems, classify software systems, and estimate development effort early in the software life-cycle to within plusmn;20% across a range of application types
  },
  Doi                      = {10.1109/32.917523},
  ISSN                     = {0098-5589},
  Keywords                 = {algebraic specification;application types;formal framework;functionality;gradient;magnitude;pilot study;problem complexity;semantic properties;software development effort estimation;software life-cycle;software metrics;software size measurement;software specification;software systems classification;syntactic properties;validation;vector prediction model;vector size measure;algebraic specification;computational complexity;size measurement;software cost estimation;software metrics;vectors;}
}

@Article{He_KDE09_Imbalance,
  Title                    = {Learning from Imbalanced Data},
  Author                   = {Haibo He and E.A. Garcia},
  Journal                  = {IEEE Transactions on Knowledge and Data Engineering},
  Year                     = {2009},

  Month                    = {Sept.},
  Number                   = {9},
  Pages                    = {1263--1284},
  Volume                   = {21},

  Abstract                 = {With the continuous expansion of data availability in many large-scale, complex, and networked systems, such as surveillance, security, Internet, and finance, it becomes critical to advance the fundamental understanding of knowledge discovery and analysis from raw data to support decision-making processes. Although existing knowledge discovery and data engineering techniques have shown great success in many real-world applications, the problem of learning from imbalanced data (the imbalanced learning problem) is a relatively new challenge that has attracted growing attention from both academia and industry. The imbalanced learning problem is concerned with the performance of learning algorithms in the presence of underrepresented data and severe class distribution skews. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. In this paper, we provide a comprehensive review of the development of research in learning from imbalanced data. Our focus is to provide a critical review of the nature of the problem, the state-of-the-art technologies, and the current assessment metrics used to evaluate learning performance under the imbalanced learning scenario. Furthermore, in order to stimulate future research in this field, we also highlight the major opportunities and challenges, as well as potential important research directions for learning from imbalanced data.},
  Doi                      = {10.1109/TKDE.2008.239},
  ISSN                     = {1041-4347},
  Keywords                 = {complex systems;data availability;data engineering;decision making;imbalanced data;knowledge discovery;large-scale systems;learning;networked systems;data mining;decision making;large-scale systems;learning (artificial intelligence);}
}

@Article{Heiat97,
  Title                    = {A model for estimating efforts required for developing small-scale business applications},
  Author                   = {Abbas Heiat and Nafisseh Heiat},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1997},
  Number                   = {1},
  Pages                    = {7--14},
  Volume                   = {39},

  Abstract                 = {Estimating the amount of effort required for developing an information system is an important project management concern. The author has developed a model that estimates small-scale software development effort in 4GL and end-user computing environments. In addition to presenting and evaluating the proposed model, this paper evaluates two of the most popular models currently used to estimate software development effort i.e., lines of code (LOC), and function points (FP). Results of the study show a significant correlation between the software development effort and all three models. Compared to LOC and FP models, the models developed in this research are less costly and easier to use in a small-scale software development environment.},
  Doi                      = {10.1016/S0164-1212(96)00159-8},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121296001598}
}

@InProceedings{herraiz2009:flossmetrics,
  Title                    = {{FLOSSMetrics}: Free / Libre / Open Source Software Metrics},
  Author                   = {Israel Herraiz and Daniel Izquierdo-Cortazar and Francisco Rivas-Hernandez and Jesus M. Gonzalez-Barahona and Gregorio Robles and Santiago Due\~nas Dominguez and Carlos Garcia-Campos and Juan Francisco Gato and Liliana Tovar},
  Booktitle                = {Proceedings of the 13th European Conference on Software Maintenance and Reengineering (CSMR)},
  Year                     = {2009},
  Publisher                = {IEEE Computer Society},

  Location                 = {Kaiserlauten, Germany}
}

@InProceedings{HRH_WETSOM12,
  Title                    = {On the Statistical Distribution of Object-Oriented System Properties},
  Author                   = {Israel Herraiz and Daniel Rodriguez and Rachel Harrison},
  Booktitle                = {3rd International Workshop on Emerging Trends in Software Metrics (WETSoM 2012)},
  Year                     = {2012},
  Month                    = {June},
  Publisher                = {IEEE}
}

@Article{HCC06,
  Title                    = {{FLOSSmole}: A Collaborative Repository for {FLOSS} Research Data and Analyses},
  Author                   = {James Howison and Megan Conklin and Kevin Crowston},
  Journal                  = {International Journal of Information Technology and Web Engineering},
  Year                     = {2006},
  Number                   = {3},
  Volume                   = {1},

  Doi                      = {10.4018/jitwe.2006070102}
}

@Article{VanHulse2009,
  Title                    = {Knowledge discovery from imbalanced and noisy data},
  Author                   = {Jason Van Hulse and Taghi Khoshgoftaar},
  Journal                  = {Data \& Knowledge Engineering},
  Year                     = {2009},
  Number                   = {12},
  Pages                    = {1513--1542},
  Volume                   = {68},

  Abstract                 = {Class imbalance and labeling errors present significant challenges to data mining and knowledge discovery applications. Some previous work has discussed these important topics, however the relationship between these two issues has not received enough attention. Further, much of the previous work in this domain is fragmented and contradictory, leading to serious questions regarding the reliability and validity of the empirical conclusions. In response to these issues, we present a comprehensive suite of experiments carefully designed to provide conclusive, reliable, and significant results on the problem of learning from noisy and imbalanced data. Noise is shown to significantly impact all of the learners considered in this work, and a particularly important factor is the class in which the noise is located (which, as discussed throughout this work, has very important implications to noise handling). The impacts of noise, however, vary dramatically depending on the learning algorithm and simple algorithms such as naive Bayes and nearest neighbor learners are often more robust than more complex learners such as support vector machines or random forests. Sampling techniques, which are often used to alleviate the adverse impacts of imbalanced data, are shown to improve the performance of learners built from noisy and imbalanced data. In particular, simple sampling techniques such as random undersampling are generally the most effective. },
  Doi                      = {http://dx.doi.org/10.1016/j.datak.2009.08.005},
  ISSN                     = {0169-023X},
  Keywords                 = {Data sampling},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0169023X09001141}
}


@Article{Japkowicz2002,
  Title                    = {The class imbalance problem: A systematic study},
  Author                   = {Japkowicz, Nathalie and Stephen, Shaju},
  Journal                  = {Intelligent Data Analysis},
  Year                     = {2002},

  Month                    = oct,
  Number                   = {5},
  Pages                    = {429--449},
  Volume                   = {6},

  ISSN                     = {1088-467X},
  Keywords                 = {C5.0,Multi-Layer Perceptrons,Support Vector Machines,class imbalances,concept learning,misclassification costs,re-sampling},
  Mendeley-groups          = {ImbalancedData},
  Publisher                = {IOS Press},
  Url                      = {http://dl.acm.org/citation.cfm?id=1293951.1293954}
}


@InProceedings{KameiEtAl:07,
  Title                    = {The effects of over and under sampling on fault--prone module detection},
  Author                   = {Y. Kamei and A. Monden and S. Matsumoto and T. Kakimoto and K. Matsumoto},
  Booktitle                = {Empirical Software Engineering and Measurement (ESEM)},
  Year                     = {2007},
  Pages                    = {196--204}
}

@Article{Kaymak_EAAI2012_AUK,
  Title                    = {The AUK: A simple alternative to the AUC},
  Author                   = {Uzay Kaymak and Arie Ben-David and Rob Potharst},
  Journal                  = {Engineering Applications of Artificial Intelligence},
  Year                     = {2012},
  Number                   = {5},
  Pages                    = {1082 - 1089},
  Volume                   = {25},

  Abstract                 = {The area under the receiver operating characteristic (ROC) curve, also known as the AUC-index, is commonly used for ranking the performance of data mining models. The AUC has various merits, such as ease of interpretation. However, since it is class indifferent, its usefulness while dealing with highly skewed data sets is questionable. In this paper, we propose a simple alternative scalar measure to the AUC-index, the area under the Kappa curve (AUK). The proposed AUK-index compensates for the class indifference of the AUC by being sensitive to the class distribution. Therefore, it is particularly suitable for measuring classifiers' performance on skewed data sets. After introducing the AUK we explore its mathematical relationship with the AUC and show that there is a non-linear relation between them.},
  Doi                      = {10.1016/j.engappai.2012.02.012},
  ISSN                     = {0952-1976},
  Keywords                 = {ROC curve},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0952197612000498}
}

@Article{Keerthi2001,
  Title                    = {Improvements to Platt's SMO Algorithm for SVM Classifier Design},
  Author                   = {S.S. Keerthi and S.K. Shevade and C. Bhattacharyya and K.R.K. Murthy},
  Journal                  = {Neural Computation},
  Year                     = {2001},
  Number                   = {3},
  Pages                    = {637-649},
  Volume                   = {13},

  File                     = {smo_mod_nc.ps.gz:http\://guppy.mpe.nus.edu.sg/\\~mpessk/svm/smo_mod_nc.ps.gz:PostScript}
}

@InProceedings{KeivanlooMSR12,
  Title                    = {A Linked Data platform for mining software repositories},
  Author                   = {Keivanloo, I. and Forbes, C. and Hmood, A. and Erfani, M. and Neal, C. and Peristerakis, G. and Rilling, J.},
  Booktitle                = {Mining Software Repositories (MSR), 2012 9th IEEE Working Conference on},
  Year                     = {2012},
  Month                    = {june},
  Pages                    = {32 -35},

  Abstract                 = {The mining of software repositories involves the extraction of both basic and value-added information from existing software repositories. The repositories will be mined to extract facts by different stakeholders (e.g. researchers, managers) and for various purposes. To avoid unnecessary pre-processing and analysis steps, sharing and integration of both basic and value-added facts are needed. In this research, we introduce SeCold, an open and collaborative platform for sharing software datasets. SeCold provides the first online software ecosystem Linked Data platform that supports data extraction and on-the-fly inter-dataset integration from major version control, issue tracking, and quality evaluation systems. In its first release, the dataset contains about two billion facts, such as source code statements, software licenses, and code clones from 18 000 software projects. In its second release the SeCold project will contain additional facts mined from issue trackers and versioning systems. Our approach is based on the same fundamental principle as Wikipedia: researchers and tool developers share analysis results obtained from their tools by publishing them as part of the SeCold portal and therefore make them an integrated part of the global knowledge domain. The SeCold project is an official member of the Linked Data dataset cloud and is currently the eighth largest online dataset available on the Web.},
  Doi                      = {10.1109/MSR.2012.6224296},
  ISSN                     = {2160-1852},
  Keywords                 = {code clones;collaborative platform;linked data platform;mining software repositories;on-the-fly inter-dataset integration;online software ecosystem linked data platform;software datasets;software licenses;software packages;software repositories;source code statements;value added information;data mining;software packages;}
}




@Article{KhoshgoftaarEtAl:05,
  Title                    = {Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study},
  Author                   = {T.M. Khoshgoftaar and N. Seliya and K. Gao},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {183-218},
  Volume                   = {10}
}

@Article{Khoshgoftaar09,
  Title                    = {Empirical Case Studies in Attribute Noise Detection},
  Author                   = {Khoshgoftaar, T.M. and Van Hulse, J.},
  Journal                  = {Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on},
  Year                     = {2009},

  Month                    = {july },
  Number                   = {4},
  Pages                    = {379 -388},
  Volume                   = {39},

  Abstract                 = {The quality of data is an important issue in any domain-specific data mining and knowledge discovery initiative. The validity of solutions produced by data-driven algorithms can be diminished if the data being analyzed are of low quality. The quality of data is often realized in terms of data noise present in the given dataset and can include noisy attributes or labeling errors. Hence, tools for improving the quality of data are important to the data mining analyst. We present a comprehensive empirical investigation of our new and innovative technique for ranking attributes in a given dataset from most to least noisy. Upon identifying the noisy attributes, specific treatments can be applied depending on how the data are to be used. In a classification setting, for example, if the class label is determined to contain the most noise, processes to cleanse this important attribute may be undertaken. Independent variables or predictors that have a low correlation to the class attribute and appear noisy may be eliminated from the analysis. Several case studies using both real-world and synthetic datasets are presented in this study. The noise detection performance is evaluated by injecting noise into multiple attributes at different noise levels. The empirical results demonstrate conclusively that our technique provides a very accurate and useful ranking of noisy attributes in a given dataset.},
  Doi                      = {10.1109/TSMCC.2009.2013815},
  ISSN                     = {1094-6977},
  Keywords                 = {attribute noise detection;data quality;domain-specific data mining;knowledge discovery;data analysis;data mining;}
}

@Article{Khoshgoftaar02,
  Title                    = {Using Regression Trees To Classify Fault-Prone Software Modules},
  Author                   = {T. M. Khoshgoftaar and E. Allen and J. Deng},
  Journal                  = {IEEE Transactions on Reliability},
  Year                     = {2002},
  Number                   = {4},
  Pages                    = {455--462},
  Volume                   = {51}
}

@Article{Khoshgoftaar97,
  Title                    = {Application of Neural Networks to Software Quality Modeling of a Very Large Telecommunications System},
  Author                   = {T. M. Khoshgoftaar and E. Allen and J. Hudepohl and S. Aud},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1997},
  Number                   = {4},
  Pages                    = {902--909},
  Volume                   = {8}
}

@InProceedings{KhoshgoftaarGKN12,
  Title                    = {Exploring an iterative feature selection technique for highly imbalanced data sets},
  Author                   = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri},
  Booktitle                = {2012 IEEE 13th International Conference on Information Reuse and Integration (IRI)},
  Year                     = {2012},
  Month                    = {aug.},
  Pages                    = {101--108},

  Abstract                 = {The quality of a classification model is affected by two factors in a training data set: (1) the presence of excessive features and (2) the presence of imbalanced distributions between two classes in a binary classification problem. This paper presents an iterative feature selection method to deal with these two problems. The proposed method consists of an iterative process of data sampling followed by feature ranking and finally aggregating the results generated during the iterative process. In this study, we investigate a number of feature ranking techniques and a data sampling method with two different post-sampling proportions between the two classes. We compare the iterative feature selection technique to the one where a data sampling and a feature ranking technique are used together but only once (without iteration). The empirical study is carried out on two groups of highly imbalanced data sets from a real-world software system. The results demonstrate that our proposed iterative feature selection technique performs on average better than the method without iteration.},
  Doi                      = {10.1109/IRI.2012.6302997}
}

@Article{Khoshgoftaar03,
  Title                    = {Analogy-Based Practical Classification Rules for Software Quality Estimation},
  Author                   = {Taghi M. Khoshgoftaar and Naeem Seliya},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2003},
  Number                   = {4},
  Pages                    = {325--350},
  Volume                   = {8},

  Address                  = {Hingham, MA, USA},
  Doi                      = {http://dx.doi.org/10.1023/A:1025316301168},
  ISSN                     = {1382-3256},
  Publisher                = {Kluwer Academic Publishers}
}

@Article{Kim2012,
  Title                    = {Classification cost: An empirical comparison among traditional classifier, Cost-Sensitive Classifier, and MetaCost},
  Author                   = {Kim, Jungeun and Choi, Keunho and Kim, Gunwoo and Suh, Yongmoo},
  Journal                  = {Expert Syst. Appl.},
  Year                     = {2012},

  Month                    = mar,
  Number                   = {4},
  Pages                    = {4013--4019},
  Volume                   = {39},

  __markedentry            = {[drg:6]},
  Acmid                    = {2076894},
  Address                  = {Tarrytown, NY, USA},
  Doi                      = {10.1016/j.eswa.2011.09.071},
  ISSN                     = {0957-4174},
  Issue_date               = {March, 2012},
  Keywords                 = {Cost-Sensitive Classifier, Cost-sensitive learning, Fraud detection, MetaCost},
  Numpages                 = {7},
  Publisher                = {Pergamon Press, Inc.},
  Url                      = {http://dx.doi.org/10.1016/j.eswa.2011.09.071}
}

@InProceedings{KS02,
  Title                    = {Case and Feature Subset Selection in Case-Based Software Project Effort Prediction},
  Author                   = {C. Kirsopp and M. Shepperd},
  Booktitle                = {Proceedings of 22nd International Conference on Knowledge-Based Systems and Applied Artificial Intelligence (SGAI'02)},
  Year                     = {2002}
}

@Article{Kitchenham2008,
  Title                    = {The role of replications in empirical software engineering: a word of warning},
  Author                   = {Kitchenham, Barbara},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {219--221},
  Volume                   = {13},

  Affiliation              = {Keele University School of Computing and Mathematics Keele ST5 5BG UK},
  ISSN                     = {1382-3256},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@Article{Kitchenham1998,
  Title                    = {A procedure for analyzing unbalanced datasets},
  Author                   = {Kitchenham, B.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {1998},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {278 -301},
  Volume                   = {24},

  Abstract                 = {This paper describes a procedure for analyzing unbalanced datasets that include many nominal- and ordinal-scale factors. Such datasets are often found in company datasets used for benchmarking and productivity assessment. The two major problems caused by lack of balance are that the impact of factors can be concealed and that spurious impacts can be observed. These effects are examined with the help of two small artificial datasets. The paper proposes a method of forward pass residual analysis to analyze such datasets. The analysis procedure is demonstrated on the artificial datasets and then applied to the COCOMO dataset. The paper ends with a discussion of the advantages and limitations of the analysis procedure},
  Doi                      = {10.1109/32.677185},
  ISSN                     = {0098-5589},
  Keywords                 = {COCOMO dataset;analysis of variance;benchmarking;forward pass residual analysis;productivity assessment;residual analysis;software metrics;statistical analysis;unbalanced datasets;software metrics;}
}

@Article{Kitchenham2002,
  Title                    = {The question of scale economies in software --- why cannot researchers agree?},
  Author                   = {Barbara A. Kitchenham},
  Journal                  = {Information and Software Technology},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {13--24},
  Volume                   = {44},

  Abstract                 = {This paper investigates the different research results obtained when different researchers have investigated the issue of economies and diseconomies of scale in software projects. Although researchers have used broadly similar sets of software project data sets, the results of their analyses and the conclusions they have drawn have differed. The paper highlights methodological differences that have lead to the conflicting results and shows how in many cases the differing results can be reconciled. It discusses the application of econometric concepts such as production frontiers and data envelopment analysis (DEA) to software data sets. It concludes that the assumptions underlying DEA may make it unsuitable for most software datasets but stochastic production frontiers may be relevant. It also raises some statistical issues that suggest testing hypothesis about economies and diseconomies of scale may be much more difficult than has been appreciated. The paper concludes with a plea for agreed standards for research synthesis activities.},
  Doi                      = {10.1016/S0950-5849(01)00204-X},
  ISSN                     = {0950-5849},
  Keywords                 = {Software estimation models},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S095058490100204X}
}

@Article{Kitchenham97,
  Title                    = {Evaluating software engineering methods and tools, part 7: planning feature analysis evaluation},
  Author                   = {Kitchenham, Barbara Ann},
  Journal                  = {SIGSOFT Software Engineering Notes},
  Year                     = {1997},

  Month                    = jul,
  Number                   = {4},
  Pages                    = {21--24},
  Volume                   = {22},

  Acmid                    = {263251},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/263244.263251},
  ISSN                     = {0163-5948},
  Issue_date               = {July 1997},
  Numpages                 = {4},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/263244.263251}
}

@Article{Kitchenham85,
  Title                    = {Software project development cost estimation},
  Author                   = {Barbara A. Kitchenham and N.R. Taylor},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1985},
  Number                   = {4},
  Pages                    = {267--278},
  Volume                   = {5},

  Abstract                 = {This paper reports the results of an empirical investigation of the relationships between effort expended, time scales, and project size for software project development. The observed relationships were compared with those predicted by Lawrence Putnam's Rayleigh curve model and Barry Boehm's COCOMO model. The results suggested that although the form of the basic empirical relationships were consistent with the cost models, the COCOMO model was a poor estimator of cost for the current data set and the data did not follow the Rayleigh curve suggested by Putnam. However, the results did suggest that it was possible to develop cost models tailored to a particular environment and to improve the precision of the models as they are used during the development cycle by including additional information such as the known effort for the early development phases. The paper finishes by discussing some of the problems involved in developing useful cost models.},
  Doi                      = {10.1016/0164-1212(85)90026-3},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0164121285900263}
}

@Article{Kitchenham02_maintenance,
  Title                    = {An empirical study of maintenance and development estimation accuracy},
  Author                   = {Barbara Kitchenham and Shari Lawrence Pfleeger and Beth McColl and Suzanne Eagan},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2002},
  Number                   = {1},
  Pages                    = {57--77},
  Volume                   = {64},

  Abstract                 = {We analyzed data from 145 maintenance and development projects managed by a single outsourcing company, including effort and duration estimates, effort and duration actuals, and function points counts. The estimates were made as part of the company's standard project estimating process that involved producing two or more estimates for each project and selecting one estimate to be the basis of client-agreed budgets. We found that effort estimates chosen as a basis for project budgets were, in general, reasonably good, with 63\% of the estimates being within 25\% of the actual value, and an average absolute error of 0.26. These estimates were significantly better than regression estimates based on adjusted function points, although the function point models were based on a homogeneous subset of the full data set, and we allowed for the fact that the model parameters changed over time. Furthermore, there was little evidence that the accuracy of the selected estimates was due to their becoming the target values for the project managers.},
  Doi                      = {10.1016/S0164-1212(02)00021-3},
  ISSN                     = {0164-1212},
  Keywords                 = {Estimation accuracy},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121202000213}
}



@InProceedings{KJ95,
  Title                    = {Automatic Parameter Selection by Minimizing Estimated Error},
  Author                   = {R. Kohavi and G.H. John},
  Booktitle                = {12th Int. Conf. on Machine Learning},
  Year                     = {1995},

  Address                  = {San Francisco},
  Pages                    = {304--312}
}

@Article{Kohavi97,
  Title                    = {Data Mining using MLC++, a Machine Learning Library in C++},
  Author                   = {Ron Kohavi and Dan Sommerfield and James Dougherty},
  Journal                  = {International Journal of Artificial Intelligence Tools},
  Year                     = {1997},
  Number                   = {4},
  Pages                    = {537--566},
  Volume                   = {6}
}

@Article{Koru05,
  Title                    = {Building Effective Defect-Prediction Models in Practice},
  Author                   = {A. G. Koru and Hongfang Liu},
  Journal                  = {IEEE Software},
  Year                     = {2005},
  Pages                    = {23-29},
  Volume                   = {22},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/MS.2005.149},
  ISSN                     = {0740-7459},
  Publisher                = {IEEE Computer Society}
}

@Article{Kotsiantis2008,
  Title                    = {Local reweight wrapper for the problem of imbalance},
  Author                   = {Kotsiantis, S. B.},
  Journal                  = {Int. J. Artif. Intell. Soft Comput.},
  Year                     = {2008},

  Month                    = nov,
  Number                   = {1},
  Pages                    = {25--38},
  Volume                   = {1},

  __markedentry            = {[drg:]},
  Acmid                    = {1460815},
  Address                  = {Inderscience Publishers, Geneva, SWITZERLAND},
  Doi                      = {10.1504/IJAISC.2008.021262},
  ISSN                     = {1755-4950},
  Issue_date               = {November 2008},
  Keywords                 = {classification, imbalanced data sets, local learning, local wrapper, predictive analysis, reweighting training, skewed class distributions, supervised machine learning},
  Numpages                 = {14},
  Publisher                = {Inderscience Publishers},
  Url                      = {http://dx.doi.org/10.1504/IJAISC.2008.021262}
}

@Article{Lopez2012,
  Title                    = {Analysis of preprocessing vs. cost-sensitive learning for imbalanced classification. Open problems on intrinsic data characteristics},
  Author                   = {L\'{o}pez, Victoria and Fern\'{a}ndez, Alberto and Moreno-Torres, Jose G. and Herrera, Francisco},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2012},

  Month                    = jun,
  Number                   = {7},
  Pages                    = {6585--6608},
  Volume                   = {39},

  Doi                      = {10.1016/j.eswa.2011.12.043},
  ISSN                     = {09574174},
  Mendeley-groups          = {ImbalancedData, Slice},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0957417411017143}
}

@Article{Lopez2014,
  Title                    = {On the importance of the validation technique for classification with imbalanced datasets: Addressing covariate shift when data is skewed},
  Author                   = {Victoria L\'opez and Alberto Fern\'andez and Francisco Herrera},
  Journal                  = {Information Sciences},
  Year                     = {2014},
  Pages                    = {1--13},
  Volume                   = {257},

  Abstract                 = {Abstract In the field of Data Mining, the estimation of the quality of the learned models is a key step in order to select the most appropriate tool for the problem to be solved. Traditionally, a k-fold validation technique has been carried out so that there is a certain degree of independency among the results for the different partitions. In this way, the highest average performance will be obtained by the most robust approach. However, applying a random division of the instances over the folds may result in a problem known as dataset shift, which consists in having a different data distribution between the training and test folds. In classification with imbalanced datasets, in which the number of instances of one class is much lower than the other class, this problem is more severe. The misclassification of minority class instances due to an incorrect learning of the real boundaries caused by a not well fitted data distribution, truly affects the measures of performance in this scenario. Regarding this fact, we propose the use of a specific validation technique for the partitioning of the data, known as Distribution optimally balanced stratified cross-validation to avoid this harmful situation in the presence of imbalance. This methodology makes the decision of placing close-by samples on different folds, so that each partition will end up with enough representatives of every region. We have selected a wide number of imbalanced datasets from KEEL dataset repository for our study, using several learning techniques from different paradigms, thus making the conclusions extracted to be independent of the underlying classifier. The analysis of the results has been carried out by means of the proper statistical study, which shows the goodness of this approach for dealing with imbalanced data.},
  Doi                      = {http://dx.doi.org/10.1016/j.ins.2013.09.038},
  ISSN                     = {0020-0255},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0020025513006804}
}

@Article{LBMP08,
  Title                    = {Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings},
  Author                   = {Lessmann, S. and Baesens, B. and Mues, C. and Pietsch, S.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2008},

  Month                    = {July-Aug.},
  Number                   = {4},
  Pages                    = {485--496},
  Volume                   = {34},

  Abstract                 = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
  Doi                      = {10.1109/TSE.2008.35},
  ISSN                     = {0098-5589},
  Keywords                 = {benchmarking classification models;code attributes;fault-prone modules;metric-based classification;predictive classification models;proprietary data sets;software defect prediction;software quality;statistical testing procedures;testing efficiency;benchmark testing;software quality;statistical testing;}
}

@InProceedings{LiR06,
  Title                    = {A comparative study of attribute weighting heuristics for effort estimation by analogy},
  Author                   = {Li, Jingzhou and Ruhe, Guenther},
  Booktitle                = {Proceedings of the 2006 ACM/IEEE international symposium on Empirical software engineering},
  Year                     = {2006},

  Address                  = {New York, NY, USA},
  Pages                    = {66--74},
  Publisher                = {ACM},
  Series                   = {ISESE '06},

  Acmid                    = {1159746},
  Doi                      = {10.1145/1159733.1159746},
  ISBN                     = {1-59593-218-6},
  Keywords                 = {attribute, estimation by analogy, missing values, non-quantitative attributes, rough set analysis, selection and weighting, software effort estimation},
  Location                 = {Rio de Janeiro, Brazil},
  Numpages                 = {9},
  Url                      = {http://doi.acm.org/10.1145/1159733.1159746}
}

@Article{LiRAR07,
  Title                    = {A flexible method for software effort estimation by analogy},
  Author                   = {Jingzhou Li and Guenther Ruhe and Ahmed Al-Emran and Michael M. Richter},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {65--106},
  Volume                   = {12},

  Abstract                 = {Effort estimation by analogy uses information from former similar projects to predict the effort for a new project. Existing analogy-based methods are limited by their inability to handle non-quantitative data and missing values. The accuracy of predictions needs improvement as well. In this paper, we propose a new flexible method called AQUA that is able to overcome the limitations of former methods. AQUA combines ideas from two known analogy-based estimation techniques: case-based reasoning and collaborative filtering. The method is applicable to predict effort related to any object at the requirement, feature, or project levels. Which are the main contributions of AQUA when compared to other methods? First, AQUA supports non-quantitative data by defining similarity measures for different data types. Second, it is able to tolerate missing values. Third, the results from an explorative study in this paper shows that the prediction accuracy is sensitive to both the number N of analogies (similar objects) taken for adaptation and the threshold T for the degree of similarity, which is true especially for larger data sets. A fixed and small number of analogies, as assumed in existing analogy-based methods, may not produce the best accuracy of prediction. Fourth, a flexible mechanism based on learning of existing data is proposed for determining the appropriate values of N and T likely to offer the best accuracy of prediction. New criteria to measure the quality of prediction are proposed. AQUA was validated against two internal and one public domain data sets with non-quantitative attributes and missing values. The obtained results are encouraging. In addition, acomparative analysis with existing analogy-based estimation methods was conducted using three publicly available data sets that were used by these methods. Intwo of the three cases, AQUA outperformed all other methods.},
  Doi                      = {10.1007/s10664-006-7552-4},
  Url                      = {http://www.springerlink.com/content/5r0262nk47v15703}
}

@InProceedings{Li+Reformat:2007,
  Title                    = {A practical method for the software fault--prediction},
  Author                   = {Z. Li and M. Reformat},
  Booktitle                = {IEEE International Conference Information Reuse and Integration (IRI)},
  Year                     = {2007},
  Pages                    = {659--666}
}

@PhdThesis{Liebchen11_PhD,
  Title                    = {Data Cleaning Techniques for Software Engineering Data Sets},
  Author                   = {G. Liebchen},
  School                   = {Dept. of Information Systems and Computing, Brunel University},
  Year                     = {2011},

  Address                  = {London},
  Type                     = {PhD Dissertation}
}

@InProceedings{Lincke:2008,
  Title                    = {Comparing software metrics tools},
  Author                   = {Lincke, R\"{u}diger and Lundberg, Jonas and L\"{o}we, Welf},
  Booktitle                = {Proceedings of the 2008 International Symposium on Software Testing and Analysis (ISSTA'08)},
  Year                     = {2008},
  Pages                    = {131--142},
  Publisher                = {ACM},

  Numpages                 = {12}
}

@Article{LBNRB09,
  Title                    = {Sourcerer: mining and searching internet-scale software repositories},
  Author                   = {Linstead, Erik and Bajracharya, Sushil and Ngo, Trung and Rigor, Paul and Lopes, Cristina and Baldi, Pierre},
  Journal                  = {Data Mining and Knowledge Discovery},
  Year                     = {2009},
  Note                     = {10.1007/s10618-008-0118-x},
  Pages                    = {300--336},
  Volume                   = {18},

  Affiliation              = {University of California Donald Bren School of Information and Computer Sciences Irvine USA},
  ISSN                     = {1384-5810},
  Issue                    = {2},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@Article{LWHS91,
  Title                    = {Organizational benchmarking using the ISBSG Data Repository},
  Author                   = {Lokan, C. and Wright, T. and Hill, P. and Stringer, M.},
  Journal                  = {IEEE Software},
  Year                     = {2001},

  Month                    = {sep/oct},
  Number                   = {5},
  Pages                    = {26 -32},
  Volume                   = {18},

  Doi                      = {10.1109/52.951491},
  ISSN                     = {0740-7459},
  Keywords                 = {ISBSG Data Repository;International Software Benchmarking Standards Group;best-practice networking;completed software projects;cost estimation;function points structure;organizational benchmarking;project benchmarking;project duration;summary analyses;software performance evaluation;software standards;}
}

@Misc{Lopes+Bajracharya+Ossher+Baldi:2010,
  Title                    = {{UCI} Source Code Data Sets},

  Author                   = {C. Lopes and S. Bajracharya and J. Ossher and P. Baldi},
  Year                     = {2010},

  Institution              = {University of California, Irvine, Bren School of Information and Computer Sciences},
  Url                      = {http://www.ics.uci.edu/$\sim$lopes/datasets/}
}

@Article{MairSJ05,
  Title                    = {An analysis of data sets used to train and validate cost prediction systems},
  Author                   = {Mair, Carolyn and Shepperd, Martin and J{\o}rgensen, Magne},
  Journal                  = {SIGSOFT Software Engineering Notes},
  Year                     = {2005},

  Month                    = may,
  Number                   = {4},
  Pages                    = {1--6},
  Volume                   = {30},

  Acmid                    = {1083166},
  Address                  = {New York, NY, USA},
  Doi                      = {10.1145/1082983.1083166},
  ISSN                     = {0163-5948},
  Issue_date               = {July 2005},
  Keywords                 = {cost prediction, data sets, standardisation},
  Numpages                 = {6},
  Publisher                = {ACM},
  Url                      = {http://doi.acm.org/10.1145/1082983.1083166}
}

@Article{Matson,
  Title                    = {Software development cost estimation using function points},
  Author                   = {Matson, J.E. and Barrett, B.E. and Mellichamp, J.M.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1994},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {275 -287},
  Volume                   = {20},

  Abstract                 = {This paper presents an assessment of several published statistical regression models that relate software development effort to software size measured in function points. The principal concern with published models has to do with the number of observations upon which the models were based and inattention to the assumptions inherent in regression analysis. The research describes appropriate statistical procedures in the context of a case study based on function point data for 104 software development projects and discusses limitations of the resulting model in estimating development effort. The paper also focuses on a problem with the current method for measuring function points that constrains the effective use of function points in regression models and suggests a modification to the approach that should enhance the accuracy of prediction models based on function points in the future},
  Doi                      = {10.1109/32.277575},
  ISSN                     = {0098-5589},
  Keywords                 = {development effort;function points;prediction models;regression analysis;software development cost estimation;statistical regression models;software cost estimation;statistical analysis;}
}

@InProceedings{Harman2014ssbse,
author="Harman, Mark and Islam, Syed and Jia, Yue and Minku, Leandro L. and Sarro, Federica and Srivisut, Komsan",
editor="Le Goues, Claire
and Yoo, Shin",
title="Less is More: Temporal Fault Predictive Performance over Multiple Hadoop Releases",
booktitle="Search-Based Software Engineering",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="240--246",
isbn="978-3-319-09940-8"
}

@Article{Matthews1975Comparison,
  Title                    = {Comparison of the predicted and observed secondary structure of T4 phage lysozyme},
  Author                   = {Matthews, B. W.},
  Journal                  = {Biochimica et biophysica acta},
  Year                     = {1975},

  Month                    = oct,
  Number                   = {2},
  Pages                    = {442--451},
  Volume                   = {405},

  Abstract                 = {Predictions of the secondary structure of T4 phage lysozyme, made by a number of investigators on the basis of the amino acid sequence, are compared with the structure of the protein determined experimentally by X-ray crystallography. Within the amino terminal half of the molecule the locations of helices predicted by a number of methods agree moderately well with the observed structure, however within the carboxyl half of the molecule the overall agreement is poor. For eleven different helix predictions, the coefficients giving the correlation between prediction and observation range from 0.14 to 0.42. The accuracy of the predictions for both beta-sheet regions and for turns are generally lower than for the helices, and in a number of instances the agreement between prediction and observation is no better than would be expected for a random selection of residues. The structural predictions for T4 phage lysozyme are much less successful than was the case for adenylate kinase (Schulz et al. (1974) Nature 250, 140-142). No one method of prediction is clearly superior to all others, and although empirical predictions based on larger numbers of known protein structure tend to be more accurate than those based on a limited sample, the improvement in accuracy is not dramatic, suggesting that the accuracy of current empirical predictive methods will not be substantially increased simply by the inclusion of more data from additional protein structure determinations.},
  ISSN                     = {0006-3002},
  Keywords                 = {classification, diplomarbeit, evaluation, ml},
  Priority                 = {2},
  Url                      = {http://view.ncbi.nlm.nih.gov/pubmed/1180967}
}

@Book{Maxwell02,
  Title                    = {Applied statistics for software managers},
  Author                   = {Katrina Maxwell},
  Publisher                = {Katrina Maxwell},
  Year                     = {2002},

  Pages                    = {333}
}

@Article{mccabe76,
  Title                    = {A Complexity Measure},
  Author                   = {T.J. McCabe},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {1976},

  Month                    = {December},
  Number                   = {4},
  Pages                    = {308--320},
  Volume                   = {2}
}

@InProceedings{Mende2010,
  Title                    = {Replication of defect prediction studies: problems, pitfalls and recommendations},
  Author                   = {Mende, Thilo},
  Booktitle                = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
  Year                     = {2010},

  Address                  = {New York, NY, USA},
  Pages                    = {5:1--5:10},
  Publisher                = {ACM},
  Series                   = {PROMISE'10},

  Acmid                    = {1868336},
  Articleno                = {5},
  Doi                      = {10.1145/1868328.1868336},
  ISBN                     = {978-1-4503-0404-7},
  Keywords                 = {defect prediction model, replication},
  Location                 = {Timisoara, Romania},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/1868328.1868336}
}

@InProceedings{Mende10,
  Title                    = {Effort-Aware Defect Prediction Models},
  Author                   = {Mende, Thilo and Koschke, Rainer},
  Booktitle                = {Proceedings of the 2010 14th European Conference on Software Maintenance and Reengineering (CSMR'10)},
  Year                     = {2010},

  Address                  = {Washington, DC, USA},
  Pages                    = {107--116},
  Publisher                = {IEEE Computer Society},
  Series                   = {CSMR'10},

  Acmid                    = {1955974},
  Doi                      = {10.1109/CSMR.2010.18},
  ISBN                     = {978-0-7695-4321-5},
  Keywords                 = {Defect Prediction Models, Evaluation, Cost-Benefits},
  Numpages                 = {10},
  Url                      = {http://dx.doi.org/10.1109/CSMR.2010.18}
}

@InProceedings{Mende09,
  Title                    = {Revisiting the evaluation of defect prediction models},
  Author                   = {Thilo Mende and Rainer Koschke},
  Booktitle                = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering (PROMISE'09)},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {1--10},
  Publisher                = {ACM},

  Doi                      = {http://doi.acm.org/10.1145/1540438.1540448},
  ISBN                     = {978-1-60558-634-2},
  Location                 = {Vancouver, British Columbia, Canada}
}

@Article{MendesMDG08,
  Title                    = {Cross-company vs. single-company web effort models using the Tukutuku database: An extended study},
  Author                   = {Emilia Mendes and Sergio {Di Martino} and Filomena Ferrucci and Carmine Gravino},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Note                     = {<ce:title>Software Process and Product Measurement</ce:title>},
  Number                   = {5},
  Pages                    = {673 - 690},
  Volume                   = {81},

  Doi                      = {10.1016/j.jss.2007.07.044},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121207002385}
}

@Misc{Menzies2012,
  Title                    = {The PROMISE Repository of empirical software engineering data},

  Author                   = {Tim Menzies and Bora Caglayan and Ekrem Kocaguneli and Joe Krall and Fayola Peters and Burak Turhan },
  Month                    = {June},
  Year                     = {2012},

  Url                      = {http://promisedata.googlecode.com}
}

@Other{promise12,
  Title                    = {The PROMISE Repository of empirical software engineering data},
  Author                   = {Tim Menzies and Bora Caglayan and Ekrem Kocaguneli and Joe Krall and Fayola Peters and Burak Turhan },
  Month                    = {June},
  Url                      = {http://promisedata.googlecode.com},
  Year                     = {2012}
}

@Article{Menzies07b,
  Title                    = {Problems with Precision: A Response to Comments on Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {Tim Menzies and Alex Dekhtyar and Justin Distefano and Jeremy Greenwald},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {637--640},
  Volume                   = {33},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70721},
  ISSN                     = {0098-5589},
  Publisher                = {IEEE Computer Society}
}

@Article{Menzies07,
  Title                    = {Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {T. Menzies and J. Greenwald and A. Frank},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},

  Optmonth                 = {January},
  Optnumber                = {1},
  Optpages                 = {2--13},
  Optvolume                = {33}
}

@Article{Misic19981,
  Title                    = {Estimation of effort and complexity: An object-oriented case study},
  Author                   = {Vojislav B Mi\v{s}i\'c and Dejan N Tev{s}i\'c},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1998},
  Number                   = {2},
  Pages                    = {133--143},
  Volume                   = {41},

  Abstract                 = {The metrication of object-oriented software systems is still an underdeveloped part within the domain of the object paradigm. An empirical investigation aimed at finding appropriate measures and establishing simple, yet usable and cost-effective models for estimation and control of object-oriented system projects, was undertaken on a set of object-oriented projects implemented in a stable environment. First, the measures available were screened for possible correlations; then, the models suitable for estimation were derived and discussed. Effort was found to correlate well with the total number of classes and the total number of methods, both of which are known at the end of the design phase. A number of other models for estimation of the source code complexity were also defined.},
  Doi                      = {10.1016/S0164-1212(97)10014-0},
  ISSN                     = {0164-1212},
  Keywords                 = {Software measurement},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121297100140}
}

@Book{Mit97,
  Title                    = {Machine Learning},
  Author                   = {T. Mitchell},
  Publisher                = {McGraw Hill},
  Year                     = {1997}
}

@Article{Miyazaki94,
  Title                    = {Robust regression for developing software estimation models},
  Author                   = {Y. Miyazaki and M. Terakado and K. Ozaki and H. Nozaki},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1994},
  Number                   = {1},
  Pages                    = {3--16},
  Volume                   = {27},

  Abstract                 = {To develop a good software estimation model fitted to actual data, the evaluation criteria of goodness of fit is necessary. The first major problem discussed here is that ordinary relative error used for this criterion is not suitable because it has a bound in the case of under-estimation and no bound in the case of overestimation. We propose use of a new relative error called balanced relative error as the basis for the criterion and introduce seven evaluation criteria for software estimation models. The second major problem is that the ordinary least-squares method used for calculation of parameter values of a software estimation model is neither consistent with the criteria nor robust enough, which means that the solution is easily distorted by outliers. We propose a new consistent and robust method called the least-squares of inverted balanced relative errors (LIRS) and demonstrates its superiority to the ordinary least-squares method by use of five actual data sets. Through the analysis of these five data sets with LIRS, we show the importance of consistent data collection and development standarization to develop a good software sizing model. We compare goodness of fit between the sizing model based on the number of screens, forms, and files, and the sizing model based on the number of data elements for each of them. Based on this comparison, the validity of the number of data elements as independent variables for a sizing model is examined. Moreover, the validity of increasing the number of independent variables is examined.},
  Doi                      = {10.1016/0164-1212(94)90110-4},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0164121294901104}
}

@Article{mockus2002,
  Title                    = {Two case studies of {O}pen {S}ource Software development: {A}pache and {M}ozilla},
  Author                   = {Audris Mockus and Roy T. Fielding and James D. Herbsleb},
  Journal                  = {ACM Transactions on Software Engineering and Methodology},
  Year                     = {2002},
  Number                   = {3},
  Pages                    = {309--346},
  Volume                   = {11}
}

@Article{Moser1999,
  Title                    = {Cost estimation based on business models},
  Author                   = {Simon Moser and Brian Henderson-Sellers and Vojislav B Mi\v{s}i\'c},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1999},
  Number                   = {1},
  Pages                    = {33--42},
  Volume                   = {49},

  Abstract                 = {Software development requires early and accurate cost estimation in order to enhance likely success. System complexity needs to be measured and then correlated with development effort. One of the best known approaches to such measurement-based estimation in the area of Information Systems is Function Point Analysis (FPA). Although it is reasonably well used in practice, FPA has been shown to be formally ambiguous and to have some serious practical deficiencies as well, mainly in the context of newly emerged object-oriented modeling approaches. This paper reports results from an empirical study undertaken in Swiss industry covering 36 projects. We observed that a new formally sound approach, the System Meter (SM) method, which explicitly takes reuse into account, predicts effort substantially better than FPA.},
  Doi                      = {10.1016/S0164-1212(99)00064-3},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121299000643}
}

@Article{MyrtveitTSE05,
  Title                    = {Reliability and validity in comparative studies of software prediction models},
  Author                   = {Myrtveit, I. and Stensrud, E. and Shepperd, M.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2005},

  Month                    = {may},
  Number                   = {5},
  Pages                    = { 380--391},
  Volume                   = {31},

  Abstract                 = { Empirical studies on software prediction models do not converge with respect to the question "which prediction model is best?" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.},
  Doi                      = {10.1109/TSE.2005.58},
  ISSN                     = {0098-5589},
  Keywords                 = { accuracy indicator; analogy estimation; arbitrary function approximators; convergence; cost estimation; cross validation; data sample; empirical method; machine learning model; regression model; simulation; software metrics; software prediction model; software reliability; software validity; convergence; function approximation; learning (artificial intelligence); program verification; regression analysis; software cost estimation; software metrics; software reliability;}
}

@InProceedings{NZZH10,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and Kim Herzig and Brendan Murphy},
  Booktitle                = {Proceedings of the 21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)},
  Year                     = {2010},
  Month                    = {November},

  Location                 = {San Jose, California, USA}
}

@InProceedings{NZ1,
  Title                    = {The Ultimate Debian Database: Consolidating bazaar metadata for Quality Assurance and data mining},
  Author                   = {Nussbaum, L. and Zacchiroli, S.},
  Booktitle                = {7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  Year                     = {2010},
  Month                    = {May},
  Pages                    = {52--61},

  Doi                      = {10.1109/MSR.2010.5463277},
  Keywords                 = {Debian project;FLOSS distribution;FLOSS project;RedHat;SQL database;SQL query;Ubuntu;bazaar development model;bazaar metadata;community driven distribution;data mining;open source software;quality assurance;SQL;data mining;marketing data processing;meta data;public domain software;quality assurance;}
}

@InProceedings{oates1997,
  Title                    = {The Effects of Training Set Size on Decision Tree Complexity},
  Author                   = {Oates, T. and Jensen, D.},
  Booktitle                = {Proceedings of the Fourteenth International Conference on Machine Learning (ICML)},
  Year                     = {1997},
  Organization             = {Citeseer},
  Pages                    = {254--262}
}

@Misc{ohloh12,
  Title                    = {Ohloh},

  Author                   = {Ohloh},

  Url                      = {http://www.ohloh.net/}
}

@Article{Ohlsson,
  Title                    = {Predicting fault-prone software modules in telephone switches},
  Author                   = {Ohlsson, N. and Alberg, H.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {1996},
  Number                   = {12},
  Pages                    = {886--894},
  Volume                   = {22},

  Abstract                 = {An empirical study was carried out at Ericsson Telecom AB to investigate the relationship between several design metrics and the number of function test failure reports associated with software modules. A tool, ERIMET, was developed to analyze the design documents automatically. Preliminary results from the study of 130 modules showed that: based on fault and design data one can satisfactorily build, before coding has started, a prediction model for identifying the most fault-prone modules. The data analyzed show that 20 percent of the most fault-prone modules account for 60 percent of all faults. The prediction model built in this paper would have identified 20 percent of the modules accounting for 47 percent of all faults. At least four design measures can alternatively be used as predictors with equivalent performance. The size (with respect to the number of lines of code) used in a previous prediction model was not significantly better than these four measures. The Alberg diagram introduced in this paper offers a way of assessing a predictor based on historical data, which is a valuable complement to linear regression when prediction data is ordinal. Applying the method described in this paper makes it possible to use measures at the design phase to predict the most fault-prone modules},
  Doi                      = {10.1109/32.553637},
  ISSN                     = {0098-5589},
  Keywords                 = {diagrams;electronic switching systems;program testing;software fault tolerance;software metrics;statistical analysis;telecommunication computing;telephony;Alberg diagram;ERIMET tool;Ericsson Telecom AB;design documents;design measures;design metrics;fault-prone software module prediction;function test failure reports;historical data;linear regression;performance;prediction model;software reliability;telephone switches;Data analysis;Fault diagnosis;Linear regression;Phase measurement;Predictive models;Size measurement;Software testing;Switches;Telecommunication switching;Telephony}
}

@InProceedings{Ostrand07,
  Title                    = {How to measure success of fault prediction models},
  Author                   = {Thomas J. Ostrand and Elaine J. Weyuker},
  Booktitle                = {SOQUA'07: Fourth international workshop on Software quality assurance},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Pages                    = {25--30},
  Publisher                = {ACM},

  Doi                      = {http://doi.acm.org/10.1145/1295074.1295080},
  ISBN                     = {978-1-59593-724-7},
  Location                 = {Dubrovnik, Croatia}
}

@Article{Peng2009,
  Title                    = {Empirical Evaluation Of Classifiers For Software Risk Management},
  Author                   = {Yi Peng and Gang Kou and Guoxun Wang and Honggang Wang and Franz Ko},
  Journal                  = {International Journal of Information Technology \& Decision Making (IJITDM)},
  Year                     = {2009},
  Number                   = {04},
  Pages                    = {749--767},
  Volume                   = {08},

  Abstract                 = {Software development involves plenty of risks, and errors exist in software modules represent a major kind of risk. Software defect prediction techniques and tools that identify software errors play a crucial role in software risk management. Among software defect prediction techniques, classification is a commonly used approach. Various types of classifiers have been applied to software defect prediction in recent years. How to select an adequate classifier (or set of classifiers) to identify error prone software modules is an important task for software development organizations. There are many different measures for classifiers and each measure is intended for assessing different aspect of a classifier. This paper developed a performance metric that combines various measures to evaluate the quality of classifiers for software defect prediction. The performance metric is analyzed experimentally using 13 classifiers on 11 public domain software defect datasets. The results of the experiment indicate that support vector machines (SVM), C4.5 algorithm, and K-nearest-neighbor algorithm ranked the top three classifiers.},
  Keywords                 = {Classification; software risk management; software defect prediction; performance metric}
}

@Article{Peng2010,
  Title                    = {User preferences based software defect detection algorithms selection using {MCDM}},
  Author                   = {Yi Peng and Guoxun Wang and Honggang Wang},
  Journal                  = {Information Sciences},
  Year                     = {2010},
  Pages                    = { - },
  Volume                   = {In Press.},

  Doi                      = {DOI: 10.1016/j.ins.2010.04.019},
  ISSN                     = {0020-0255},
  Url                      = {http://www.sciencedirect.com/science/article/B6V0C-4YXK4KM-1/2/0841843c022cfd6b78886eb45bc8ccf0}
}

@Article{Perini_TSE12,
  Title                    = {A Machine Learning Approach to Software Requirements Prioritization},
  Author                   = {Perini, A. and Susi, A. and Avesani, P.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2012},
  Number                   = {--},
  Pages                    = {--},
  Volume                   = {Preprint},

  Abstract                 = {Deciding which, among a set of requirements, are to be considered first and in which order, is a strategic process in software development. This task is commonly referred as requirements prioritization. This paper describes a requirements prioritization method, called CBRank, which combines project's stakeholders preferences with requirements ordering approximations computed through Machine Learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the art prioritization method, providing evidence of the method ability to support the management of the trade-off between elicitation effort and ranking accuracy, and to exploit domain knowledge. A case study on a real software project complement these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefit and limits of the method.},
  Doi                      = {10.1109/TSE.2012.52},
  ISSN                     = {0098-5589},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InCollection{Platt1998,
  Title                    = {Fast Training of Support Vector Machines using Sequential Minimal Optimization},
  Author                   = {J. Platt},
  Booktitle                = {Advances in Kernel Methods - Support Vector Learning},
  Publisher                = {MIT Press},
  Year                     = {1998},
  Editor                   = {B. Schoelkopf and C. Burges and A. Smola},

  File                     = {smo-book.pdf:http\://research.microsoft.com/\\~jplatt/smo-book.pdf:PDF;smo-book.ps.gz:http\://research.microsoft.com/\\~jplatt/smo-book.ps.gz:PostScript},
  Url                      = {http://research.microsoft.com/\~jplatt/smo.html}
}

@Book{Quinlan1993,
  Title                    = {C4.5: Programs for machine learning},
  Author                   = {J.R. Quinlan},
  Publisher                = {Morgan Kaufmann},
  Year                     = {1993},

  Address                  = {San Mateo, California}
}

@Article{Raiha10,
  Title                    = {A Survey on Search-based Software Design},
  Author                   = {Outi R\"aih\"a},
  Journal                  = {Computer Science Review},
  Year                     = {2010},

  Month                    = {November},
  Number                   = {4},
  Pages                    = {203--249},
  Volume                   = {4},

  Doi                      = {http://dx.doi.org/10.1016/j.cosrev.2010.06.001},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@InProceedings{Rakotomalala05,
  Title                    = {TANAGRA : un logiciel gratuit pour l'enseignement et la recherche},
  Author                   = {Ricco Rakotomalala},
  Booktitle                = {Actes de EGC'2005, RNTI-E-3},
  Year                     = {2005},
  Pages                    = {697--702},
  Volume                   = {2}
}

@InProceedings{Ratzinger07_Refactoring,
  Title                    = {Mining Software Evolution to Predict Refactoring},
  Author                   = {Ratzinger, J. and Sigmund, T. and Vorburger, P. and Gall, H.},
  Booktitle                = {Empirical Software Engineering and Measurement, 2007. ESEM 2007. First International Symposium on},
  Year                     = {2007},
  Month                    = {sept.},
  Pages                    = {354 -363},

  Abstract                 = {Can we predict locations of future refactoring based on the development history? In an empirical study of open source projects we found that attributes of software evolution data can be used to predict the need for refactoring in the following two months of development. Information systems utilized in software projects provide a broad range of data for decision support. Versioning systems log each activity during the development, which we use to extract data mining features such as growth measures, relationships between classes, the number of authors working on a particular piece of code, etc. We use this information as input into classification algorithms to create prediction models for future refactoring activities. Different state-of-the-art classifiers are investigated such as decision trees, logistic model trees, prepositional rule learners, and nearest neighbor algorithms. With both high precision and high recall we can assess the refactoring proneness of object-oriented systems. Although we investigate different domains, we discovered critical factors within the development life cycle leading to refactoring, which are common among all studied projects.},
  Doi                      = {10.1109/ESEM.2007.9},
  ISSN                     = {1938-6451},
  Keywords                 = {data mining features;decision support;decision trees;development history;logistic model trees;mining software evolution;object-oriented systems;open source projects;prepositional rule learners;software projects;versioning systems;data mining;decision trees;object-oriented methods;software engineering;},
  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{RaudysJ91,
  Title                    = {Small sample size effects in statistical pattern recognition: recommendations for practitioners},
  Author                   = {Raudys, S.J. and Jain, A.K.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {1991},

  Month                    = {mar},
  Number                   = {3},
  Pages                    = {252--264},
  Volume                   = {13},

  Doi                      = {10.1109/34.75512},
  ISSN                     = {0162-8828},
  Keywords                 = {classifiers;error estimation;error rates;feature selection;learning;sample size effects;statistical pattern recognition;pattern recognition;statistical analysis;}
}

@InProceedings{Rob10,
  Title                    = {Replicating {MSR}: A study of the potential replicability of papers published in the Mining Software Repositories proceedings},
  Author                   = {Robles, G.},
  Booktitle                = {7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
  Year                     = {2010},
  Month                    = {may},
  Pages                    = {171--180},

  Doi                      = {10.1109/MSR.2010.5463348},
  Keywords                 = {MSR;MSR replication;data sources;mining software repositories proceedings;potential replicability;software projects;data mining;}
}

@Article{RGIH09,
  Title                    = {Tools for the study of the usual data sources found in libre software projects},
  Author                   = {Gregorio Robles and Jesus M. Gonzalez-Barahona and Daniel Izquierdo-Cortazar and Israel Herraiz},
  Journal                  = {International Journal of Open Source Software and Processes},
  Year                     = {2009},

  Month                    = {Jan-March},
  Number                   = {1},
  Pages                    = {24--45},
  Volume                   = {1}
}

@Article{RodriguezEtAlINS12,
  Title                    = {Searching for rules to detect defective modules: A subgroup discovery approach},
  Author                   = {D. Rodr\'iguez and R. Ruiz and J.C. Riquelme and J.S. Aguilar-Ruiz},
  Journal                  = {Information Sciences},
  Year                     = {2012},
  Number                   = {0},
  Pages                    = {14--30},
  Volume                   = {191},

  Doi                      = {10.1016/j.ins.2011.01.039},
  ISSN                     = {0020-0255},
  Keywords                 = {Defect prediction}
}

@Article{RodriguezEtAl12,
  Title                    = {Empirical findings on team size and productivity in software development},
  Author                   = {D. Rodriguez and M.A. Sicilia and E. Garcia and R. Harrison},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2012},
  Number                   = {3},
  Pages                    = {562--570},
  Volume                   = {85},

  Doi                      = {10.1016/j.jss.2011.09.009},
  ISSN                     = {0164-1212},
  Keywords                 = {Team size}
}

@Article{RodJJ2006,
  Title                    = {Rotation Forest: A New Classifier Ensemble Method},
  Author                   = {Rodriguez, J.J. and Kuncheva, L.I. and Alonso, C.J.},
  Journal                  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  Year                     = {2006},

  Month                    = {oct},
  Number                   = {10},
  Pages                    = {1619--1630},
  Volume                   = {28},

  Abstract                 = {We propose a method for generating classifier ensembles based on feature extraction. To create the training data for a base classifier, the feature set is randomly split into K subsets (K is a parameter of the algorithm) and principal component analysis (PCA) is applied to each subset. All principal components are retained in order to preserve the variability information in the data. Thus, K axis rotations take place to form the new features for a base classifier. The idea of the rotation approach is to encourage simultaneously individual accuracy and diversity within the ensemble. Diversity is promoted through the feature extraction for each base classifier. Decision trees were chosen here because they are sensitive to rotation of the feature axes, hence the name "forest". Accuracy is sought by keeping all principal components and also using the whole data set to train each base classifier. Using WEKA, we examined the rotation forest ensemble on a random selection of 33 benchmark data sets from the UCI repository and compared it with bagging, AdaBoost, and random forest. The results were favorable to rotation forest and prompted an investigation into diversity-accuracy landscape of the ensemble models. Diversity-error diagrams revealed that rotation forest ensembles construct individual classifiers which are more accurate than these in AdaBoost and random forest, and more diverse than these in bagging, sometimes more accurate as well},
  Doi                      = {10.1109/TPAMI.2006.211},
  ISSN                     = {0162-8828},
  Keywords                 = {Bagging;Classification tree analysis;Computer Society;Decision trees;Feature extraction;Machine learning;Pattern recognition;Principal component analysis;Training data;Voting;decision trees;feature extraction;pattern classification;principal component analysis;AdaBoost;bagging;classifier ensemble method;decision trees;diversity-accuracy landscape;feature extraction;principal component analysis;rotation forest;AdaBoost;Classifier ensembles;PCA;bagging;feature extraction;kappa-error diagrams.;random forest;}
}

@PhdThesis{Schofield98PhD,
  Title                    = {An Empirical investigation into software effort estimation by analogy},
  Author                   = {C. Schofield},
  School                   = {Bournemouth University},
  Year                     = {1998},

  Owner                    = {drg},
  Timestamp                = {2012.09.15}
}

@Article{Seiffert2009,
  Title                    = {Improving Software-Quality Predictions With Data Sampling and Boosting},
  Author                   = {Seiffert, C. and Khoshgoftaar, T.M. and Van Hulse, J.},
  Journal                  = {Systems, Man and Cybernetics, Part A: Systems and Humans, IEEE Transactions on},
  Year                     = {2009},
  Number                   = {6},
  Pages                    = {1283-1294},
  Volume                   = {39},

  Abstract                 = {Software-quality data sets tend to fall victim to the class-imbalance problem that plagues so many other application domains. The majority of faults in a software system, particularly high-assurance systems, usually lie in a very small percentage of the software modules. This imbalance between the number of fault-prone (fp) and non-fp (nfp) modules can have a severely negative impact on a data-mining technique's ability to differentiate between the two. This paper addresses the class-imbalance problem as it pertains to the domain of software-quality prediction. We present a comprehensive empirical study examining two different methodologies, data sampling and boosting, for improving the performance of decision-tree models designed to identify fp software modules. This paper applies five data-sampling techniques and boosting to 15 software-quality data sets of different sizes and levels of imbalance. Nearly 50 000 models were built for the experiments contained in this paper. Our results show that while data-sampling techniques are very effective in improving the performance of such models, boosting almost always outperforms even the best data-sampling techniques. This significant result, which, to our knowledge, has not been previously reported, has important consequences for practitioners developing software-quality classification models.},
  Doi                      = {10.1109/TSMCA.2009.2027131},
  ISSN                     = {1083-4427},
  Keywords                 = {data mining;decision trees;software architecture;software quality;data boosting;data mining;data sampling;decision-tree models;fault-prone modules;non-fp modules;software modules;software quality data sets;software system;Binary classification;boosting;class imbalance;classification;sampling;software quality}
}

@Article{Seiffert2010,
  Title                    = {{RUSBoost}: A Hybrid Approach to Alleviating Class Imbalance},
  Author                   = {Seiffert, C. and Khoshgoftaar, T.M. and Van Hulse, J. and Napolitano, A.},
  Journal                  = {IEEE Transactions on Systems, Man and Cybernetics, Part A: Systems and Humans},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {185--197},
  Volume                   = {40},

  Abstract                 = {Class imbalance is a problem that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and four evaluation metrics. RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data.},
  Doi                      = {10.1109/TSMCA.2009.2029559},
  ISSN                     = {1083-4427},
  Keywords                 = {data mining;learning (artificial intelligence);pattern classification;RUSBoost;class imbalance;data boosting;data mining algorithms;data sampling;sampling/boosting algorithm;skewed training data;suboptimal classification models;Binary classification;RUSBoost;boosting;class imbalance;sampling}
}

@Article{Shang2012,
  Title                    = {Using Pig as a data preparation language for large-scale mining software repositories studies: An experience report},
  Author                   = {Weiyi Shang and Bram Adams and Ahmed E. Hassan},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2012},
  Note                     = {<ce:title>Automated Software Evolution</ce:title>},
  Number                   = {10},
  Pages                    = {2195--2204},
  Volume                   = {85},

  Abstract                 = {The Mining Software Repositories (MSR) field analyzes software repository data to uncover knowledge and assist development of ever growing, complex systems. However, existing approaches and platforms for MSR analysis face many challenges when performing large-scale MSR studies. Such approaches and platforms rarely scale easily out of the box. Instead, they often require custom scaling tricks and designs that are costly to maintain and that are not reusable for other types of analysis. We believe that the web community has faced many of these software engineering scaling challenges before, as web analyses have to cope with the enormous growth of web data. In this paper, we report on our experience in using a web-scale platform (i.e., Pig) as a data preparation language to aid large-scale MSR studies. Through three case studies, we carefully validate the use of this web platform to prepare (i.e., Extract, Transform, and Load, ETL) data for further analysis. Despite several limitations, we still encourage MSR researchers to leverage Pig in their large-scale studies because of Pig's scalability and flexibility. Our experience report will help other researchers who want to scale their analyses.},
  Doi                      = {10.1016/j.jss.2011.07.034},
  ISSN                     = {0164-1212},
  Keywords                 = {Software engineering},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121211002007}
}

@Article{Shepperd_TSE01,
  Title                    = {Predicting with sparse data},
  Author                   = {Shepperd, M. and Cartwright, M.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {2001},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {987 -998},
  Volume                   = {27},

  Abstract                 = {It is well-known that effective prediction of project cost related factors is an important aspect of software engineering. Unfortunately, despite extensive research over more than 30 years, this remains a significant problem for many practitioners. A major obstacle is the absence of reliable and systematic historic data, yet this is a sine qua non for almost all proposed methods: statistical, machine learning or calibration of existing models. The authors describe our sparse data method (SDM) based upon a pairwise comparison technique and T.L. Saaty's (1980) Analytic Hierarchy Process (AHP). Our minimum data requirement is a single known point. The technique is supported by a software tool known as DataSalvage. We show, for data from two companies, how our approach, based upon expert judgement, adds value to expert judgement by producing significantly more accurate and less biased results. A sensitivity analysis shows that our approach is robust to pairwise comparison errors. We then describe the results of a small usability trial with a practicing project manager. From this empirical work, we conclude that the technique is promising and may help overcome some of the present barriers to effective project prediction},
  Doi                      = {10.1109/32.965339},
  ISSN                     = {0098-5589},
  Keywords                 = {AHP;Analytic Hierarchy Process;DataSalvage;SDM;expert judgement;minimum data requirement;pairwise comparison errors;pairwise comparison technique;practicing project manager;project cost related factor prediction;project prediction;sensitivity analysis;single known point;software engineering;software project effort;software tool;sparse data;sparse data method;systematic historic data;usability trial;data analysis;software cost estimation;software reliability;software tools;}
}

@Article{Shepperd01,
  Title                    = {Comparing software prediction techniques using simulation},
  Author                   = {Shepperd, M. and Kadoda, G.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2001},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {1014 --1022},
  Volume                   = {27},

  Abstract                 = {The need for accurate software prediction systems increases as software becomes much larger and more complex. We believe that the underlying characteristics: size, number of features, type of distribution, etc., of the data set influence the choice of the prediction system to be used. For this reason, we would like to control the characteristics of such data sets in order to systematically explore the relationship between accuracy, choice of prediction system, and data set characteristic. It would also be useful to have a large validation data set. Our solution is to simulate data allowing both control and the possibility of large (1000) validation cases. The authors compare four prediction techniques: regression, rule induction, nearest neighbor (a form of case-based reasoning), and neural nets. The results suggest that there are significant differences depending upon the characteristics of the data set. Consequently, researchers should consider prediction context when evaluating competing prediction systems. We observed that the more "messy" the data and the more complex the relationship with the dependent variable, the more variability in the results. In the more complex cases, we observed significantly different results depending upon the particular training set that has been sampled from the underlying data set. However, our most important result is that it is more fruitful to ask which is the best prediction system in a particular context rather than which is the "best" prediction system},
  Doi                      = {10.1109/32.965341},
  ISSN                     = {0098-5589},
  Keywords                 = {case-based reasoning;data set characteristics;machine learning;nearest neighbor;neural nets;prediction problem;regression;rule induction;simulation;small data sets;software prediction systems;software prediction technique comparison;training set;case-based reasoning;learning (artificial intelligence);neural nets;software metrics;virtual machines;}
}

@Article{Shepperd2012,
  Title                    = {Evaluating prediction systems in software project estimation},
  Author                   = {Martin Shepperd and Steve MacDonell},
  Journal                  = {Information and Software Technology},
  Year                     = {2012},
  Pages                    = {-},

  Doi                      = {10.1016/j.infsof.2011.12.008},
  ISSN                     = {0950-5849}
}

@Article{Shepperd97_Analogy,
  Title                    = {Estimating software project effort using analogies},
  Author                   = {Shepperd, M. and Schofield, C.},
  Journal                  = {Software Engineering, IEEE Transactions on},
  Year                     = {1997},

  Month                    = {nov},
  Number                   = {11},
  Pages                    = {736 -743},
  Volume                   = {23},

  Abstract                 = {Accurate project effort prediction is an important goal for the software engineering community. To date most work has focused upon building algorithmic models of effort, for example COCOMO. These can be calibrated to local environments. We describe an alternative approach to estimation based upon the use of analogies. The underlying principle is to characterize projects in terms of features (for example, the number of interfaces, the development method or the size of the functional requirements document). Completed projects are stored and then the problem becomes one of finding the most similar projects to the one for which a prediction is required. Similarity is defined as Euclidean distance in n-dimensional space where n is the number of project features. Each dimension is standardized so all dimensions have equal weight. The known effort values of the nearest neighbors to the new project are then used as the basis for the prediction. The process is automated using a PC-based tool known as ANGEL. The method is validated on nine different industrial datasets (a total of 275 projects) and in all cases analogy outperforms algorithmic models based upon stepwise regression. From this work we argue that estimation by analogy is a viable technique that, at the very least, can be used by project managers to complement current estimation techniques},
  Doi                      = {10.1109/32.637387},
  ISSN                     = {0098-5589},
  Keywords                 = {ANGEL;COCOMO;Euclidean distance;algorithmic models;estimation by analogy;functional requirements document;industrial datasets;nearest neighbors;personal computer-based tool;project effort prediction;project management;software development method;software engineering;software project effort estimation;stepwise regression;project management;software cost estimation;software development management;software metrics;software tools;}
}

@Article{Shepperd2013,
  Title                    = {Data Quality: Some Comments on the NASA Software Defect Datasets},
  Author                   = {Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2013},
  Number                   = {9},
  Pages                    = {1208--1215},
  Volume                   = {39},

  Abstract                 = {Background--Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective--This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method--We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results--We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions--It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners.},
  Doi                      = {10.1109/TSE.2013.11},
  ISSN                     = {0098-5589},
  Keywords                 = {data analysis;learning (artificial intelligence);pattern classification;software reliability;IEEE Transactions on Software Engineering;NASA software defect dataset;National Aeronautics and Space Administration;data preprocessing;data quality;data replication;dataset provenance;defect-prone classification;machine learning;not-defect-prone classification;software module classification;Abstracts;Communities;Educational institutions;NASA;PROM;Software;Sun;Empirical software engineering;data quality;defect prediction;machine learning}
}

@Article{daSilva_ESE12,
  Title                    = {Replication of empirical studies in software engineering research: a systematic mapping study},
  Author                   = {Fabio Q. B. da Silva and Marcos Suassuna and A. C?sar C. Fran?a and Alicia M. Grubb and Tatiana B. Gouveia and Cleviton V. F. Monteiro and Igor Ebrahim dos Santos},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012}
}

@InProceedings{QualitasCorpus:APSEC:2010,
  Title                    = {Qualitas Corpus: A Curated Collection of Java Code for Empirical Studies},
  Author                   = {Tempero, Ewan and Anslow, Craig and Dietrich, Jens and Han, Ted and Li, Jing and Lumpe, Markus and Melton, Hayden and Noble, James},
  Booktitle                = {2010 Asia Pacific Software Engineering Conference (APSEC2010)},
  Year                     = {2010},
  Month                    = {Dec}
}

@InProceedings{Thai-Nghe11,
  Title                    = {A new evaluation measure for learning from imbalanced data},
  Author                   = {Nguyen Thai-Nghe and Gantner, Z. and Schmidt-Thieme, L.},
  Booktitle                = {The 2011 International Joint Conference on Neural Networks (IJCNN'11)},
  Year                     = {2011},
  Month                    = {31 2011-aug. 5},
  Pages                    = {537 -542},

  Abstract                 = {Recently, researchers have shown that the Area Under the ROC Curve (AUC) has a serious deficiency since it implicitly uses different misclassification cost distributions for different classifiers. Thus, using the AUC can be compared to using different metrics to evaluate different classifiers [1]. To overcome this incoherence, the H measure was proposed, which uses a symmetric Beta distribution to replace the implicit cost weight distribution in the AUC. When learning from imbalanced data, misclassifying a minority class example is much more serious than misclassifying a majority class example. To take different misclassification costs into account, we propose using an asymmetric Beta distribution (B42) instead of a symmetric one. Experimental results on 36 imbalanced data sets using SVMs and logistic regression show that B42 is a good choice for evaluating on imbalanced data sets because it puts more weight on the minority class. We also show that balanced random undersampling does not work for large and highly imbalanced data sets, although it has been reported to be effective for small data sets.},
  Doi                      = {10.1109/IJCNN.2011.6033267},
  ISSN                     = {2161-4393},
  Keywords                 = {AUC;Area Under the ROC Curve;Beta distribution;H measuremenht;SVM;imbalanced data learning;logistic regression;support vector machine;data handling;learning (artificial intelligence);}
}

@Article{TurhanESE12,
  Title                    = {On the dataset shift problem in software engineering prediction models},
  Author                   = {Turhan, Burak},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012},
  Note                     = {10.1007/s10664-011-9182-8},
  Pages                    = {62--74},
  Volume                   = {17},

  Affiliation              = {Department of Information Processing Science, University of Oulu, POB.3000, 90014 Oulu, Finland},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@Conference{Van-Antwerp:2008,
  Title                    = {Advances in the SourceForge Research Data Archive (SRDA)},
  Author                   = {Van Antwerp, M. and Madey, G.},
  Booktitle                = {Fourth International Conference on Open Source Systems, IFIP 2.13 (WoPDaSD 2008)},
  Year                     = {2008},

  Address                  = {Milan, Italy},
  Month                    = {September}
}

@InProceedings{VanHulse2007,
  Title                    = {Experimental perspectives on learning from imbalanced data},
  Author                   = {Van Hulse, Jason and Khoshgoftaar, Taghi M. and Napolitano, Amri},
  Booktitle                = {Proceedings of the 24th international conference on Machine learning (ICLM)},
  Year                     = {2007},

  Address                  = {New York, NY, USA},
  Pages                    = {935--942},
  Publisher                = {ACM},
  Series                   = {ICML '07},

  Acmid                    = {1273614},
  Doi                      = {10.1145/1273496.1273614},
  ISBN                     = {978-1-59593-793-3},
  Location                 = {Corvalis, Oregon},
  Numpages                 = {8},
  Url                      = {http://doi.acm.org/10.1145/1273496.1273614}
}

@Article{Vandecruys2008,
  Title                    = {Mining software repositories for comprehensible software fault prediction models},
  Author                   = {Olivier Vandecruys and David Martens and Bart Baesens and Christophe Mues and Manu {De Backer} and Raf Haesen},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {823--839},
  Volume                   = {81},

  Abstract                 = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.},
  Doi                      = {http://dx.doi.org/10.1016/j.jss.2007.07.034},
  ISSN                     = {0164-1212},
  Keywords                 = {Classification},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0164121207001902}
}

@PhdThesis{Vasa2010,
  Title                    = {Growth and Change Dynamics in Open Source Software Systems},
  Author                   = {Rajesh Vasa},
  School                   = {Faculty of Information and Communication Technologies Swinburne University of Technology Melbourne, Australia},
  Year                     = {2010}
}

@Misc{Helix10a,
  Title                    = {Helix - Software Evolution Data Set},

  Author                   = {Rajesh Vasa and Markus Lumpe and and Allan Jones},
  HowPublished             = {\url{http://http://www.ict.swin.edu.au/research/projects/helix}},
  Year                     = {2010},

  Url                      = {http://www.ict.swin.edu.au/research/projects/helix}
}

@Article{Webb2000,
  Title                    = {MultiBoosting: A Technique for Combining Boosting and Wagging},
  Author                   = {Geoffrey I. Webb},
  Journal                  = {Machine Learning},
  Year                     = {2000},
  Number                   = {No.2},
  Volume                   = {Vol.40},

  Address                  = {Boston},
  Publisher                = {Kluwer Academic Publishers}
}

@InProceedings{Weimer09,
  Title                    = {Automatically finding patches using genetic programming},
  Author                   = {Weimer, Westley and Nguyen, ThanhVu and Le Goues, Claire and Forrest, Stephanie},
  Booktitle                = {Proceedings of the 31st International Conference on Software Engineering(ICSE'09)},
  Year                     = {2009},

  Address                  = {Washington, DC, USA},
  Pages                    = {364--374},
  Publisher                = {IEEE Computer Society},
  Series                   = {ICSE'09},

  Acmid                    = {1555051},
  Doi                      = {10.1109/ICSE.2009.5070536},
  ISBN                     = {978-1-4244-3453-4},
  Numpages                 = {11},
  Owner                    = {drg},
  Timestamp                = {2012.09.15},
  Url                      = {http://dx.doi.org/10.1109/ICSE.2009.5070536}
}

@Article{Weiss03,
  Title                    = {Learning when Training Data are Costly: The Effect of Class Distribution on Tree Induction},
  Author                   = {Gary M. Weiss and Foster Provost},
  Journal                  = {Journal Of Artificial Intelligence Research},
  Year                     = {2003},
  Pages                    = {315--354},
  Volume                   = {19}
}

@Article{Wen12_SLREffEst,
  Title                    = {Systematic literature review of machine learning based software development effort estimation models},
  Author                   = {Jianfeng Wen and Shixian Li and Zhiyong Lin and Yong Hu and Changqin Huang},
  Journal                  = {Information and Software Technology},
  Year                     = {2012},
  Number                   = {1},
  Pages                    = {41--59},
  Volume                   = {54},

  Abstract                 = {Context Software development effort estimation (SDEE) is the process of predicting the effort required to develop a software system. In order to improve estimation accuracy, many researchers have proposed machine learning (ML) based SDEE models (ML models) since 1990s. However, there has been no attempt to analyze the empirical evidence on ML models in a systematic way. Objective This research aims to systematically analyze ML models from four aspects: type of ML technique, estimation accuracy, model comparison, and estimation context. Method We performed a systematic literature review of empirical studies on ML model published in the last two decades (1991-2010). Results We have identified 84 primary studies relevant to the objective of this research. After investigating these studies, we found that eight types of ML techniques have been employed in SDEE models. Overall speaking, the estimation accuracy of these ML models is close to the acceptable level and is better than that of non-ML models. Furthermore, different ML models have different strengths and weaknesses and thus favor different estimation contexts. Conclusion ML models are promising in the field of SDEE. However, the application of ML models in industry is still limited, so that more effort and incentives are needed to facilitate the application of ML models. To this end, based on the findings of this review, we provide recommendations for researchers as well as guidelines for practitioners.},
  Doi                      = {10.1016/j.infsof.2011.09.002},
  ISSN                     = {0950-5849},
  Keywords                 = {Software effort estimation},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584911001832}
}

@Article{Williams09,
  Title                    = {Rattle: A Data Mining GUI for R},
  Author                   = {G. Williams},
  Journal                  = {The R Journal},
  Year                     = {2009},
  Number                   = {2},
  Pages                    = {45--55},
  Volume                   = {2}
}

@Article{wilson1972asymptotic,
  Title                    = {Asymptotic properties of nearest neighbor rules using edited data},
  Author                   = {Wilson, D.L.},
  Journal                  = {IEEE Transactions on Systems, Man and Cybernetics},
  Year                     = {1972},
  Number                   = {3},
  Pages                    = {408--421},

  Publisher                = {IEEE}
}

@Book{WFH11,
  Title                    = {Data Mining: Practical Machine Learning Tools and Techniques (Third Edition)},
  Author                   = {Ian H. Witten and Eibe Frank and Mark A. Hall},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2011}
}

@Article{Woodfield81,
  Title                    = {A study of several metrics for programming effort},
  Author                   = {S.N. Woodfield and V.Y. Shen and H.E. Dunsmore},
  Journal                  = {Journal of Systems and Software},
  Year                     = {1981},
  Number                   = {2},
  Pages                    = {97--103},
  Volume                   = {2},

  Abstract                 = {As the cost of programming becomes a major component of the cost of computer systems, it becomes imperative that program development and maintenance be better managed. One measurement a manager could use is programming complexity. Such a measure can be very useful if the manager is confident that the higher the complexity measure is for a programming project, the more effort it takes to complete the project and perhaps to maintain it. Until recently most measures of complexity were based only on intuition and experience. In the past 3 years two objective metrics have been introduced, McCabe's cyclomatic number v(G) and Halstead's effort measure E. This paper reports an empirical study designed to compare these two metrics with a classic size measure, lines of code. A fourth metric based on a model of programming is introduced and shown to be better than the previously known metrics for some experimental data.},
  Doi                      = {10.1016/0164-1212(81)90029-7},
  ISSN                     = {0164-1212},
  Url                      = {http://www.sciencedirect.com/science/article/pii/0164121281900297}
}

@Article{Xie_Compt09_DMSE,
  Title                    = {Data Mining for Software Engineering},
  Author                   = {Xie, Tao and Thummalapenta, Suresh and Lo, David and Liu, Chao},
  Journal                  = {Computer},
  Year                     = {2009},

  Month                    = aug,
  Number                   = {8},
  Pages                    = {55--62},
  Volume                   = {42},

  Acmid                    = {1608617},
  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {10.1109/MC.2009.256},
  ISSN                     = {0018-9162},
  Issue_date               = {August 2009},
  Keywords                 = {Computational intelligence, Data mining, Data mining, Software engineering, Design and test, Computational intelligence, Design and test, Software engineering},
  Numpages                 = {8},
  Owner                    = {drg},
  Publisher                = {IEEE Computer Society Press},
  Timestamp                = {2012.09.15},
  Url                      = {http://dx.doi.org/10.1109/MC.2009.256}
}

@Article{4407730,
  Title                    = {On the Distribution of Software Faults},
  Author                   = {Hongyu Zhang},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2008},

  Month                    = {march-april },
  Number                   = {2},
  Pages                    = {301--302},
  Volume                   = {34},

  Doi                      = {10.1109/TSE.2007.70771},
  ISSN                     = {0098-5589},
  Keywords                 = {Pareto principle;Weibull distribution;large software systems;software faults distribution;Pareto analysis;Weibull distribution;}
}

@Article{Zhang07,
  Title                    = {Comments on "Data Mining Static Code Attributes to Learn Defect Predictors"},
  Author                   = {Hongyu Zhang and Xiuzhen Zhang},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {635--637},
  Volume                   = {33},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70706},
  ISSN                     = {0098-5589},
  Publisher                = {IEEE Computer Society}
}

@Article{ZhangHFM11,
  Title                    = {Comparing the Performance of Metaheuristics for the Analysis of Multi-stakeholder Tradeoffs in Requirements Optimisation},
  Author                   = {Yuanyuan Zhang and Mark Harman and Anthony Finkelstein and S. Afshin Mansouri},
  Journal                  = {Information and Software Technology},
  Year                     = {2011},

  Month                    = {July},
  Number                   = {7},
  Pages                    = {761-773},
  Volume                   = {53},

  Doi                      = {http://dx.doi.org/10.1016/j.infsof.2011.02.001}
}

@InProceedings{Zimmermann07Eclipse,
  Title                    = {Predicting Defects for Eclipse},
  Author                   = {Zimmermann, T. and Premraj, R. and Zeller, A.},
  Booktitle                = {International Workshop on Predictor Models in Software Engineering (PROMISE'07)},
  Year                     = {2007},
  Month                    = {may},
  Pages                    = {9},

  Abstract                 = {We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
  Doi                      = {10.1109/PROMISE.2007.10},
  Keywords                 = {Eclipse;bug database;common complexity metrics;defect prediction models;open-source projects;source code locations;program debugging;public domain software;}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@misc{OverUnderIbm2019,
    author       = {IBM Corporation},
    title        = {Removing Unfair Bias in Machine Learning},
    year         = {2019},
    pages        = {1--27}
}