%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Generic template for TFC/TFM/TFG/Tesis
%
% $Id: introduccion.tex,v 1.19 2015/02/24 23:21:54 macias Exp $
%
% By:
%  + Javier Macías-Guarasa. 
%    Departamento de Electrónica
%    Universidad de Alcalá
%  + Roberto Barra-Chicote. 
%    Departamento de Ingeniería Electrónica
%    Universidad Politécnica de Madrid   
% 
% Based on original sources by Roberto Barra, Manuel Ocaña, Jesús Nuevo,
% Pedro Revenga, Fernando Herránz and Noelia Hernández. Thanks a lot to
% all of them, and to the many anonymous contributors found (thanks to
% google) that provided help in setting all this up.
%
% See also the additionalContributors.txt file to check the name of
% additional contributors to this work.
%
% If you think you can add pieces of relevant/useful examples,
% improvements, please contact us at (macias@depeca.uah.es)
%
% Copyleft 2013
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}\label{chp:intro}

\begin{FraseCelebre}
  \begin{Frase}
    Desocupado lector, sin juramento me podrás creer que quisiera que este
    libro [...] fuera el más hermoso, el más gallardo y más discreto que
    pudiera imaginarse\footnote{Tomado de ejemplos del proyecto \texis{}.}.
  \end{Frase}
  \begin{Fuente}
    Miguel de Cervantes, Don Quijote de la Mancha
  \end{Fuente}
\end{FraseCelebre}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction to Machine Learning}

Through time, humans wanted to keep all memories or thoughts precious to them
for posterity. Thanks to some technological advancements, larges amounts of 
information can be stored for a relatively cheap price - videos, photographs, 
drawings, weather measures, financial information, academic data, etc. It can 
all be digitalized and kept \textit{forever}. 

These new possibilities also brought new challenges. \textit{What to do with all 
that information?} Data scientist were born to try to manage and make some sense
out of the overwhelming new information being created.

The figure of a data scientist is that who uses scientific methods, processes, 
algorithms and systems to extract knowledge from structured and unstructured 
data. The algorithms used in data science, are sequences of statistical 
processing steps. Then, there is Machine Learning which uses computer algorithms 
that improve through experience - it is also a subset of Artificial Intelligence 
(AI). In this case, the algorithms are \textit{trained} to filter and 
separate patterns and features with massive amounts of data. 

The training and learning processes allow to make predictions and decisions for 
new data, being advantageous for almost any field: commercial purposes, fraud 
prediction, plant caring, network traffic, and so on.

The machine learning tools applied to this project are numerous, and they are
going to be further explained in the \textit{Background} section (see 
Section~\ref{chp:background}). As a brief introduction to the techniques and 
material in this paper, it has been used \textit{ECoL} R package to obtain the 
complexity metrics of several datasets; the Python's \textit{sklearn} package 
for learning algorithms and some analytical metrics (used through the 
experiments); as well as the \textit{imbalanced-learn} Python package to deal 
with imbalanced datasets. The resulting scripts allow the visualization between 
different tables and - plots for comparing results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Aim and Objectives}

Here we analyze the complexity metrics proposed by Ho and Basu~\cite{HoB2002} 
in a number of software defect datasets, that have been previously implemented 
in \textit{ECoL} R package.

The aim of this work is to explore complexity metrics on software defect 
datasets. Also, to analyze how classification algorithms are affected by 
techniques that mitigate imbalance and how that affects the complexity metrics 
previously analyzed. To do so, we explore several objectives:

\begin{itemize}
    \item RQ Which complexity metrics are related to miss-classification?
    \item RQ How complexity metrics are correlated to the outcome of supervised 
    algorithms?
    \item RQ How complexity metrics and imbalance are related? 
    \item RQ Do complexity metrics tell us something about the quality of the 
    datasets?
\end{itemize}

The purpose of this dissertation is to conduct a study of complexity metrics
and imbalanced datasets. A comparison between the \textit{raw} data and 
that data after performing some changes that should affect the results: 
K-folding Cross Validation, Imbalanced techniques, etc.

%%% Local Variables:
%%% TeX-master: "../book"
%%% End:
