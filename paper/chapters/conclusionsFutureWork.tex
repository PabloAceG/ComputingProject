%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Generic template for TFC/TFM/TFG/Tesis
%
% $Id: conclusiones.tex,v 1.3 2014/01/08 22:56:04 macias Exp $
%
% By:
%  + Javier Macías-Guarasa. 
%    Departamento de Electrónica
%    Universidad de Alcalá
%  + Roberto Barra-Chicote. 
%    Departamento de Ingeniería Electrónica
%    Universidad Politécnica de Madrid   
% 
% Based on original sources by Roberto Barra, Manuel Ocaña, Jesús Nuevo,
% Pedro Revenga, Fernando Herránz and Noelia Hernández. Thanks a lot to
% all of them, and to the many anonymous contributors found (thanks to
% google) that provided help in setting all this up.
%
% See also the additionalContributors.txt file to check the name of
% additional contributors to this work.
%
% If you think you can add pieces of relevant/useful examples,
% improvements, please contact us at (macias@depeca.uah.es)
%
% Copyleft 2013
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusions and Future Work}\label{sec:conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

In this work, we analysed data complexity metrics, as well as analytical metrics 
obtained from the confusion matrix and some basic classifiers. To recapitulate, 
the following work was conduced through this dissertation:

\begin{enumerate}
    \item An introduction to \textit{data complexity metrics}, class imbalance 
	and techniques to mitigate its effects, datasets used for the 
	experimentation, and analytical metrics (see Chapter~\ref{chp:background}, 
	Sections~\ref{sec:datasets} and \ref{sec:ev-metrics}).
    \item Python implementation of the experimentation (connecting to R's 
	\texttt{ECol} to obtain the complexity metrics) - see 
	Section~\ref{sec:results}:
    \begin{itemize}
        \item Obtain the complexity metrics of the datasets.
        \item Calculate the analytical metrics of the datasets using K-fold.
        \item Compute the analytical metrics of the datasets after applying 
        Under/Oversampling to each fold.
    \end{itemize}
    \item Analysis of the obtained results (see Section~\ref{sec:compresults}).
\end{enumerate}

The experiments carried out showed that the datasets selected have many 
\textit{problems} including \textit{imbalanced classes}, and data
\textit{overlapping} are some of them. This could be one of the reasons
why the classifiers do not achieve the expected results.

It has also been tried out a few way to improve the classification in those 
datasets, such as K-folding Cross Validation combined with Undersampling and
Oversampling techniques. The result of those experiments led to the intuition
that either the applied techniques are not fit for the datasets (do not manage
to increase the performance); or that the classification algorithms selected
do not perform as well as they were expected. To solve this, in addition to 
further research, better datasets should be generated, trying to reduce the 
value of metrics such as overlapping, and therefore maximize linearity; reduce 
the class imbalance, etc. We also need to explore other balancing techniques 
than the ones used for this paper.

In summary, we observed that the nature of the data might affect the results and 
this needs further explored in several path as stated next.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}\label{sec:futureWork}

There are several paths that can follow this work. First we need to run all 
experiments with more datasets - trying to have a wider variety of the values
obtained in the complexity metrics - and classification algorithms, that could
lead to better results than the ones used. Other way would be to apply other 
imbalance techniques could be applied to the datasets.

The final objective is to try minimize/maximize the aforementioned complexity
metrics, which should lead to better classification results.

There are also other paths to follow, for example, analyze the data/quality in 
the context of feature selection. There is also another path to to do so with 
interpretable machine learning \cite{molnar2019}.


%%% Local Variables:
%%% TeX-master: "../book"
%%% End:
