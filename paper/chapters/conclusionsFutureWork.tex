%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Generic template for TFC/TFM/TFG/Tesis
%
% $Id: conclusiones.tex,v 1.3 2014/01/08 22:56:04 macias Exp $
%
% By:
%  + Javier Macías-Guarasa. 
%    Departamento de Electrónica
%    Universidad de Alcalá
%  + Roberto Barra-Chicote. 
%    Departamento de Ingeniería Electrónica
%    Universidad Politécnica de Madrid   
% 
% Based on original sources by Roberto Barra, Manuel Ocaña, Jesús Nuevo,
% Pedro Revenga, Fernando Herránz and Noelia Hernández. Thanks a lot to
% all of them, and to the many anonymous contributors found (thanks to
% google) that provided help in setting all this up.
%
% See also the additionalContributors.txt file to check the name of
% additional contributors to this work.
%
% If you think you can add pieces of relevant/useful examples,
% improvements, please contact us at (macias@depeca.uah.es)
%
% Copyleft 2013
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusions and Future Work}\label{sec:conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

In this work, we analyzed complexity metrics, analytical metrics obtained from 
the confusion matrix and some basic classifiers.

To recapitulate, the following work was conduced through this dissertation:

\begin{enumerate}
    \item An introduction to Data complexity metrics, class imbalance and 
    techniques to mitigate its effects, datasets used for the experimentation,
    and analytical metrics (see Chapter~\ref{chp:background}, 
    Sections~\ref{sec:datasets} and \ref{sec:ev-metrics}).
    \item Python implementation of the experimentation (connecting to R's ECol
    to obtain the complexity metrics) - see Section~\ref{sec:results}:
    \begin{itemize}
        \item Obtain the complexity metrics of the datasets.
        \item Calculate the analytical metrics of the datasets using K-fold.
        \item Compute the analytical metrics of the datasets after applying 
        Under/Oversampling to each fold.
    \end{itemize}
    \item Analysis of the obtained results (see Section~\ref{sec:compresults}).
\end{enumerate}

The experiments carried out show that the datasets selected have many 
\textit{problems} that go with them: \textit{imbalanced classes}, or 
\textit{data overlapping} are some of them. This could be one of the reasons
why the classifiers do not give the expected results.

It has also been tried out a few way to improve the classification in those 
datasets, such as K-folding Cross Validation combined with Undersampling and
Oversampling techniques. The result of those experiments led to the intuition
that either the applied techniques are not fit to the datasets (do not manage
to increase the performance); or that the classification algorithms selected
do not perform as they were expected.

To solve this, better datasets could be selected/generated, trying to reduce
the value of metrics such as overlapping, and therefore maximize linearity;
or reduce the imbalance of the class (see \ref{sec:futureWork} to know more
proposals).Other option would be to use other Under/Oversampling techniques 
than the ones used for this paper.

That means that the nature of the data might affect the results. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}\label{sec:futureWork}

There are several paths that can follow this work. First we need to run all 
experiments with more datasets - trying to have a wider variety of the values
obtained in the complexity metrics - and classification algorithms, that could
lead to better results than the ones used. Other way would be to apply other 
imbalance techniques could be applied to the datasets.

The final objective is to try minimize/maximize the aforementioned complexity
metrics, which should lead to better classification results.

There are also other paths to follow, for example, analyze the data/quality in 
the context of feature selection. There is also another path to to do so with 
interpretable machine learning \cite{molnar2019}.


%%% Local Variables:
%%% TeX-master: "../book"
%%% End:


